{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macssd/anaconda3/lib/python3.6/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_9.4.1) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading train\n",
      "loading test\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Based on Vladimir Iglovikov' method: \n",
    "    https://www.kaggle.com/iglovikov/allstate-claims-severity/xgb-1114/discussion\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import time\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print('loading train')\n",
    "train = pd.read_csv('../../data/train.csv')\n",
    "print('loading test')\n",
    "test = pd.read_csv('../../data/test.csv')\n",
    "test['loss'] = np.nan\n",
    "joined = pd.concat([train, test])\n",
    "def logregobj(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    con =2\n",
    "    x =preds-labels\n",
    "    grad =con*x / (np.abs(x)+con)\n",
    "    hess =con**2 / (np.abs(x)+con)**2\n",
    "    return grad, hess \n",
    "\n",
    "\n",
    "def evalerror(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    return 'mae', mean_absolute_error(np.exp(preds), np.exp(labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/116 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 116/116 [00:45<00:00,  2.56it/s]\n"
     ]
    }
   ],
   "source": [
    "print('setting features')\n",
    "\n",
    "for column in tqdm(list(train.select_dtypes(include=['object']).columns)):\n",
    "    if train[column].nunique() != test[column].nunique():\n",
    "        set_train = set(train[column].unique())\n",
    "        set_test = set(test[column].unique())\n",
    "        remove_train = set_train - set_test\n",
    "        remove_test = set_test - set_train\n",
    "\n",
    "        remove = remove_train.union(remove_test)\n",
    "        def filter_cat(x):\n",
    "            if x in remove:\n",
    "                return np.nan\n",
    "            return x\n",
    "\n",
    "        joined[column] = joined[column].apply(lambda x: filter_cat(x), 1)\n",
    "\n",
    "    joined[column] = pd.factorize(joined[column].values, sort=True)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train = joined[joined['loss'].notnull()]\n",
    "test = joined[joined['loss'].isnull()]\n",
    "print('log loss')\n",
    "shift = 200\n",
    "#y = train['loss']\n",
    "y = np.log(train['loss'] + shift)\n",
    "ids = test['id']\n",
    "X = train.drop(['loss', 'id'], 1)\n",
    "X_test = test.drop(['loss', 'id'], 1)\n",
    "\n",
    "categorical_columns = [c for c in train.columns if ('cat' in c)]\n",
    "categorical_columns\n",
    "\n",
    "param = {'num_leaves': 200,\n",
    "     'min_data_in_leaf': 9,\n",
    "     'num_iterations': 20000,\n",
    "     'num_thread': 4,\n",
    "     'early_stopping_round': 200,\n",
    "     'objective':'regression', # notice: the default value is regression\n",
    "     'max_depth': -1,\n",
    "     'learning_rate': 0.002,\n",
    "     \"boosting\": \"gbdt\",\n",
    "     \"feature_fraction\": 0.3149,\n",
    "     \"bagging_freq\": 100,\n",
    "     \"bagging_fraction\": 0.8 ,\n",
    "     \"bagging_seed\": 2019,\n",
    "     \"metric\": 'l1',\n",
    "     \"lambda_l1\": 0.1,\n",
    "     \"random_state\": 2019,\n",
    "     \"verbosity\": -1         \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_cv = [11328]\n",
    "\n",
    "for i in range(100):\n",
    "    print('##### round {:d}#####'.format(i))\n",
    "    print('spliting data')\n",
    "    time0 = round(time.time())\n",
    "    x_train, x_valid, y_train, y_valid =train_test_split(X, y, test_size=0.1, random_state=time0)\n",
    "    d_train = lgb.Dataset(x_train, label=y_train, categorical_feature = categorical_columns)\n",
    "    d_valid = lgb.Dataset(x_valid, label=y_valid, categorical_feature = categorical_columns)\n",
    "\n",
    "    print('start training')\n",
    "    num_round = 10000\n",
    "    \n",
    "    model = lgb.train(param, d_train, num_round, valid_sets = [d_train, d_valid], verbose_eval=500)\n",
    "    gc.collect()\n",
    "\n",
    "    print('calculating CV')\n",
    "    oof  = np.exp(model.predict(x_valid, num_iteration=model.best_iteration)) - shift\n",
    "    cv = mean_absolute_error(np.exp(y_valid)-shift, oof)\n",
    "    print(\"CV score: {:<8.5f}\".format(cv))\n",
    "    \n",
    "    if len(lst_cv) < 10 or cv < lst_cv[9]:\n",
    "        lst_cv = sorted(lst_cv + [cv])\n",
    "        print('start predicting')\n",
    "        prediction = np.exp(model.predict(X_test)) - shift\n",
    "        print('preparing output')\n",
    "        submission = pd.DataFrame()\n",
    "        submission['loss'] = prediction\n",
    "        submission['id'] = ids\n",
    "        tm = str(time0) + '_' + str(round(cv*10))\n",
    "        print('time: ',tm)\n",
    "        submission.to_csv('submit_'+ tm +'.csv', index=False)\n",
    "        #submission.to_csv('submit_'+ tm +'.csv.gz', compression='gzip', index=False)\n",
    "\n",
    "        '''pck = open('pretrained_'+ tm +'.pkl', 'wb')\n",
    "        pickle.dump(model, pck)\n",
    "        pck.close()'''\n",
    "print(lst_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'print(\\'spliting data\\')\\ntime0 = round(time.time())\\ntime0 = 1549662358\\nx_train, x_valid, y_train, y_valid =train_test_split(X, y, test_size=0.1, random_state=time0)\\nd_train = lgb.Dataset(x_train, label=y_train, categorical_feature = categorical_columns)\\nd_valid = lgb.Dataset(x_valid, label=y_valid, categorical_feature = categorical_columns)\\n\\nprint(\\'start training\\')\\nnum_round = 10000\\n\\nmodel = lgb.train(param, d_train, num_round, valid_sets = [d_train, d_valid], verbose_eval=500)\\ngc.collect()\\n\\nprint(\\'calculating CV\\')\\noof  = np.exp(model.predict(x_valid, num_iteration=model.best_iteration)) - shift\\ncv = mean_absolute_error(np.exp(y_valid)-shift, oof)\\nprint(\"CV score: {:<8.5f}\".format(cv))'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "print('spliting data')\n",
    "time0 = round(time.time())\n",
    "time0 = 1549662358\n",
    "x_train, x_valid, y_train, y_valid =train_test_split(X, y, test_size=0.1, random_state=time0)\n",
    "d_train = lgb.Dataset(x_train, label=y_train, categorical_feature = categorical_columns)\n",
    "d_valid = lgb.Dataset(x_valid, label=y_valid, categorical_feature = categorical_columns)\n",
    "\n",
    "print('start training')\n",
    "num_round = 10000\n",
    "\n",
    "model = lgb.train(param, d_train, num_round, valid_sets = [d_train, d_valid], verbose_eval=500)\n",
    "gc.collect()\n",
    "\n",
    "print('calculating CV')\n",
    "oof  = np.exp(model.predict(x_valid, num_iteration=model.best_iteration)) - shift\n",
    "cv = mean_absolute_error(np.exp(y_valid)-shift, oof)\n",
    "print(\"CV score: {:<8.5f}\".format(cv))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### without using categorical_feature = categorical_columns\n",
    "[9983]\ttraining's l1: 0.322764\tvalid_1's l1: 0.366759\n",
    "CV score: 1136.30073\n",
    "\n",
    "### with: categorical_feature = categorical_columns\n",
    "[8945]\ttraining's l1: 0.31769\tvalid_1's l1: 0.370906\n",
    "CV score: 1132.81941\n",
    "\n",
    "### with t0 = 1549661848\n",
    "CV score: 1129.79278\n",
    "\n",
    "### with t0 = 1549662358\n",
    "CV score: 1123.17918"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run the model 100 times, and record the random_state (t0) and cv scores.\n",
    "\n",
    "##### round 0#####\n",
    "CV score: 1129.79278\n",
    "1549661848 1129.7927782815498\n",
    "##### round 1#####\n",
    "CV score: 1136.51924\n",
    "1549662123 1136.5192416569341\n",
    "##### round 2#####\n",
    "CV score: 1123.17918\n",
    "1549662358 1123.179182642565\n",
    "##### round 3#####\n",
    "CV score: 1132.07467\n",
    "1549662604 1132.0746656433714\n",
    "##### round 4#####\n",
    "CV score: 1135.50915\n",
    "1549662799 1135.5091455530624\n",
    "##### round 5#####\n",
    "CV score: 1137.78329\n",
    "1549663022 1137.783288284275"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1549661848: 1129.7927782815498, 1549662123: 1136.5192416569341, 1549662358: 1123.179182642565, 1549662604: 1132.0746656433714, 1549662799: 1135.5091455530624, 1549663022: 1137.783288284275, 1549663248: 1114.6283538761509, 1549663495: 1107.54382844742, 1549663736: 1153.052261414065, 1549663969: 1139.2455491007345, 1549664216: 1140.2105253885306, 1549664481: 1139.2710443417081, 1549664733: 1127.5950969440396, 1549664978: 1147.7933065604948, 1549665207: 1147.649831426128, 1549665471: 1124.7687147852087, 1549665742: 1149.9824618573034, 1549665996: 1125.2262189515152, 1549666246: 1132.9058776181228, 1549666476: 1136.4724553095907, 1549666707: 1148.2555535277206, 1549666957: 1136.569249619499, 1549667198: 1126.0090995993924, 1549667467: 1162.0702028292685, 1549667705: 1133.5121320955172, 1549667947: 1140.888875923199, 1549668171: 1132.9828559170223, 1549668422: 1133.1794257586664, 1549668675: 1147.7777698624245, 1549668906: 1155.4665076823535, 1549669143: 1115.5866950100128, 1549669364: 1140.5195131887958, 1549669617: 1136.6244551833472, 1549669866: 1133.908417677121, 1549670102: 1129.423172570854, 1549670339: 1147.4496858720265, 1549670571: 1137.0533669567892, 1549670781: 1130.7857311513103, 1549671057: 1142.990339057888, 1549671285: 1150.3471536847806, 1549671506: 1131.819821249843, 1549671732: 1129.3019364871882, 1549671965: 1101.2077218618037, 1549672195: 1128.1893806316752, 1549672451: 1136.6308608815402, 1549672693: 1140.1028917443286, 1549672934: 1131.1012016650996, 1549673163: 1130.765416921753, 1549673425: 1137.8186753068742, 1549673685: 1134.760583840955, 1549673911: 1129.7336027519686, 1549674124: 1129.4861310687277, 1549674353: 1126.1354856748346, 1549674591: 1140.5796015372405, 1549674814: 1125.327229744304, 1549675050: 1141.3335850329597, 1549675293: 1135.0124558756347, 1549675546: 1132.9766202987578, 1549675782: 1117.3292056910147, 1549676016: 1144.5908353097984, 1549676268: 1121.423938382672, 1549676521: 1150.6104864919216, 1549676764: 1131.686373125918, 1549677023: 1121.9492446281802, 1549677235: 1132.2872576711893, 1549677489: 1139.0388533070923, 1549677739: 1121.014471312103, 1549678047: 1127.6281282490536, 1549678286: 1137.6471425607929, 1549678513: 1136.2114190919146, 1549678747: 1139.9149879007841, 1549679000: 1130.4781849599506, 1549679232: 1135.9937553310324, 1549679462: 1131.2850889140288, 1549679694: 1129.8149029876697, 1549679886: 1155.9973488618975, 1549680136: 1134.990493534167, 1549680354: 1118.8780890290184, 1549680602: 1138.4436874220237, 1549680837: 1128.096142401457, 1549681046: 1120.45433460368, 1549681289: 1133.6560847727676, 1549681530: 1130.1051115233677, 1549681780: 1151.9549966402337, 1549682010: 1126.084652904825, 1549682267: 1134.294828153496, 1549682516: 1140.4705588891486, 1549682774: 1118.8597725723334, 1549683014: 1130.877817305895, 1549683277: 1131.6623021282692, 1549683571: 1112.9327088227114, 1549683827: 1119.2518437220297, 1549684083: 1135.1710798131771, 1549684332: 1144.5817537097466, 1549684589: 1141.185544002736, 1549684866: 1138.9067029137632, 1549685176: 1142.758048913006, 1549685472: 1141.3313794507685, 1549685733: 1133.8266243998605, 1549686045: 1139.3454119287899}\n",
      "[(1549671965, 1101.2077218618037), (1549663495, 1107.54382844742), (1549683571, 1112.9327088227114), (1549663248, 1114.6283538761509), (1549669143, 1115.5866950100128), (1549675782, 1117.3292056910147), (1549682774, 1118.8597725723334), (1549680354, 1118.8780890290184), (1549683827, 1119.2518437220297), (1549681046, 1120.45433460368), (1549677739, 1121.014471312103), (1549676268, 1121.423938382672), (1549677023, 1121.9492446281802), (1549662358, 1123.179182642565), (1549665471, 1124.7687147852087), (1549665996, 1125.2262189515152), (1549674814, 1125.327229744304), (1549667198, 1126.0090995993924), (1549682010, 1126.084652904825), (1549674353, 1126.1354856748346), (1549664733, 1127.5950969440396), (1549678047, 1127.6281282490536), (1549680837, 1128.096142401457), (1549672195, 1128.1893806316752), (1549671732, 1129.3019364871882), (1549670102, 1129.423172570854), (1549674124, 1129.4861310687277), (1549673911, 1129.7336027519686), (1549661848, 1129.7927782815498), (1549679694, 1129.8149029876697), (1549681530, 1130.1051115233677), (1549679000, 1130.4781849599506), (1549673163, 1130.765416921753), (1549670781, 1130.7857311513103), (1549683014, 1130.877817305895), (1549672934, 1131.1012016650996), (1549679462, 1131.2850889140288), (1549683277, 1131.6623021282692), (1549676764, 1131.686373125918), (1549671506, 1131.819821249843), (1549662604, 1132.0746656433714), (1549677235, 1132.2872576711893), (1549666246, 1132.9058776181228), (1549675546, 1132.9766202987578), (1549668171, 1132.9828559170223), (1549668422, 1133.1794257586664), (1549667705, 1133.5121320955172), (1549681289, 1133.6560847727676), (1549685733, 1133.8266243998605), (1549669866, 1133.908417677121), (1549682267, 1134.294828153496), (1549673685, 1134.760583840955), (1549680136, 1134.990493534167), (1549675293, 1135.0124558756347), (1549684083, 1135.1710798131771), (1549662799, 1135.5091455530624), (1549679232, 1135.9937553310324), (1549678513, 1136.2114190919146), (1549666476, 1136.4724553095907), (1549662123, 1136.5192416569341), (1549666957, 1136.569249619499), (1549669617, 1136.6244551833472), (1549672451, 1136.6308608815402), (1549670571, 1137.0533669567892), (1549678286, 1137.6471425607929), (1549663022, 1137.783288284275), (1549673425, 1137.8186753068742), (1549680602, 1138.4436874220237), (1549684866, 1138.9067029137632), (1549677489, 1139.0388533070923), (1549663969, 1139.2455491007345), (1549664481, 1139.2710443417081), (1549686045, 1139.3454119287899), (1549678747, 1139.9149879007841), (1549672693, 1140.1028917443286), (1549664216, 1140.2105253885306), (1549682516, 1140.4705588891486), (1549669364, 1140.5195131887958), (1549674591, 1140.5796015372405), (1549667947, 1140.888875923199), (1549684589, 1141.185544002736), (1549685472, 1141.3313794507685), (1549675050, 1141.3335850329597), (1549685176, 1142.758048913006), (1549671057, 1142.990339057888), (1549684332, 1144.5817537097466), (1549676016, 1144.5908353097984), (1549670339, 1147.4496858720265), (1549665207, 1147.649831426128), (1549668675, 1147.7777698624245), (1549664978, 1147.7933065604948), (1549666707, 1148.2555535277206), (1549665742, 1149.9824618573034), (1549671285, 1150.3471536847806), (1549676521, 1150.6104864919216), (1549681780, 1151.9549966402337), (1549663736, 1153.052261414065), (1549668906, 1155.4665076823535), (1549679886, 1155.9973488618975), (1549667467, 1162.0702028292685)]\n"
     ]
    }
   ],
   "source": [
    "# sort by the cv score\n",
    "\n",
    "with open('output.txt', 'r') as f:\n",
    "    i = 1\n",
    "    dct = {}\n",
    "    for line in f:\n",
    "        if i%3 == 0: \n",
    "            [a, b] = line.split(' ')\n",
    "            dct[int(a)] = float(b)\n",
    "        i += 1\n",
    "    print(dct)\n",
    "    top = sorted(dct.items(), key=lambda kv: kv[1], reverse = False)\n",
    "    print(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below are the top 10 cv score\n",
    "\n",
    "top_10 = [(1549671965, 1101.2077218618037), (1549663495, 1107.54382844742), (1549683571, 1112.9327088227114), (1549663248, 1114.6283538761509), (1549669143, 1115.5866950100128), (1549675782, 1117.3292056910147), (1549682774, 1118.8597725723334), (1549680354, 1118.8780890290184), (1549683827, 1119.2518437220297), (1549681046, 1120.45433460368)]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
