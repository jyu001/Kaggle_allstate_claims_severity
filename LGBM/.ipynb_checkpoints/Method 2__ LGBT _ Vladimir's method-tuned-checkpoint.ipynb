{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macssd/anaconda3/lib/python3.6/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_9.4.1) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading train\n",
      "loading test\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Based on Vladimir Iglovikov' method: \n",
    "    https://www.kaggle.com/iglovikov/allstate-claims-severity/xgb-1114/discussion\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import time\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print('loading train')\n",
    "train = pd.read_csv('../../data/train.csv')\n",
    "print('loading test')\n",
    "test = pd.read_csv('../../data/test.csv')\n",
    "test['loss'] = np.nan\n",
    "joined = pd.concat([train, test])\n",
    "def logregobj(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    con =2\n",
    "    x =preds-labels\n",
    "    grad =con*x / (np.abs(x)+con)\n",
    "    hess =con**2 / (np.abs(x)+con)**2\n",
    "    return grad, hess \n",
    "\n",
    "\n",
    "def evalerror(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    return 'mae', mean_absolute_error(np.exp(preds), np.exp(labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/116 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 116/116 [00:47<00:00,  2.42it/s]\n"
     ]
    }
   ],
   "source": [
    "print('setting features')\n",
    "\n",
    "for column in tqdm(list(train.select_dtypes(include=['object']).columns)):\n",
    "    if train[column].nunique() != test[column].nunique():\n",
    "        set_train = set(train[column].unique())\n",
    "        set_test = set(test[column].unique())\n",
    "        remove_train = set_train - set_test\n",
    "        remove_test = set_test - set_train\n",
    "\n",
    "        remove = remove_train.union(remove_test)\n",
    "        def filter_cat(x):\n",
    "            if x in remove:\n",
    "                return np.nan\n",
    "            return x\n",
    "\n",
    "        joined[column] = joined[column].apply(lambda x: filter_cat(x), 1)\n",
    "\n",
    "    joined[column] = pd.factorize(joined[column].values, sort=True)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train = joined[joined['loss'].notnull()]\n",
    "test = joined[joined['loss'].isnull()]\n",
    "print('log loss')\n",
    "shift = 200\n",
    "#y = train['loss']\n",
    "y = np.log(train['loss'] + shift)\n",
    "ids = test['id']\n",
    "X = train.drop(['loss', 'id'], 1)\n",
    "X_test = test.drop(['loss', 'id'], 1)\n",
    "\n",
    "categorical_columns = [c for c in train.columns if ('cat' in c)]\n",
    "categorical_columns\n",
    "\n",
    "param = {'num_leaves': 200,\n",
    "     'min_data_in_leaf': 9,\n",
    "     'num_iterations': 50000,\n",
    "     'num_thread': 4,\n",
    "     'early_stopping_round': 200,\n",
    "     'objective':'regression', # notice: the default value is regression\n",
    "     'max_depth': -1,\n",
    "     'learning_rate': 0.002,\n",
    "     \"boosting\": \"gbdt\",\n",
    "     \"feature_fraction\": 0.3149,\n",
    "     \"bagging_freq\": 100,\n",
    "     \"bagging_fraction\": 0.8 ,\n",
    "     \"bagging_seed\": 2019,\n",
    "     \"metric\": 'l1',\n",
    "     \"lambda_l1\": 0.1,\n",
    "     \"random_state\": 2019,\n",
    "     \"verbosity\": -1         \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct = {}\n",
    "\n",
    "for i in range(100):\n",
    "    print('##### round {:d}#####'.format(i))\n",
    "    print('spliting data')\n",
    "    time0 = round(time.time())\n",
    "    x_train, x_valid, y_train, y_valid =train_test_split(X, y, test_size=0.1, random_state=time0)\n",
    "    d_train = lgb.Dataset(x_train, label=y_train, categorical_feature = categorical_columns)\n",
    "    d_valid = lgb.Dataset(x_valid, label=y_valid, categorical_feature = categorical_columns)\n",
    "\n",
    "    print('start training')\n",
    "    num_round = 10000\n",
    "    model = lgb.train(param, d_train, num_round, valid_sets = [d_train, d_valid], verbose_eval=500)\n",
    "    gc.collect()\n",
    "\n",
    "    print('calculating CV')\n",
    "    oof  = np.exp(model.predict(x_valid, num_iteration=model.best_iteration)) - shift\n",
    "    cv = mean_absolute_error(np.exp(y_valid)-shift, oof)\n",
    "    print(\"CV score: {:d}, {:<8.5f}\".format(dpth, cv))\n",
    "    \n",
    "    dct[time0] = cv\n",
    "    \n",
    "    '''\n",
    "    if len(lst_cv) < 10 or cv < lst_cv[9]:\n",
    "        lst_cv = sorted(lst_cv + [cv])\n",
    "        print('start predicting')\n",
    "        prediction = np.exp(model.predict(X_test)) - shift\n",
    "        print('preparing output')\n",
    "        submission = pd.DataFrame()\n",
    "        submission['loss'] = prediction\n",
    "        submission['id'] = ids\n",
    "        tm = str(time0) + '_' + str(round(cv*10))\n",
    "        print('time: ',tm)\n",
    "        submission.to_csv('submit_'+ tm +'.csv', index=False)\n",
    "        #submission.to_csv('submit_'+ tm +'.csv.gz', compression='gzip', index=False)\n",
    "\n",
    "        pck = open('pretrained_'+ tm +'.pkl', 'wb')\n",
    "        pickle.dump(model, pck)\n",
    "        pck.close()\n",
    "    '''\n",
    "    \n",
    "top = sorted(dct.items(), key=lambda kv: kv[1], reverse = False)\n",
    "print(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by the cv score\n",
    "\n",
    "with open('output.txt', 'r') as f:\n",
    "    i = 1\n",
    "    dct = {}\n",
    "    for line in f:\n",
    "        if i%3 == 0: \n",
    "            [a, b] = line.split(' ')\n",
    "            dct[int(a)] = float(b)\n",
    "        i += 1\n",
    "    print(dct)\n",
    "    top = sorted(dct.items(), key=lambda kv: kv[1], reverse = False)\n",
    "    print(top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter tuning needed\n",
    "\n",
    "### using max_depth = -1 and found overfitting\n",
    "### for different random_state in train/valid split, got different CV scores from 1101 to 1140. (run the model 100 times, and record the random_state (t0) and cv scores.)\n",
    "\n",
    "    time0 = round(time.time())\n",
    "    x_train, x_valid, y_train, y_valid =train_test_split(X, y, test_size=0.1, random_state=time0)\n",
    "\n",
    "top_10 = [(1549671965, 1101.2077218618037), (1549663495, 1107.54382844742), (1549683571, 1112.9327088227114), (1549663248, 1114.6283538761509), (1549669143, 1115.5866950100128), (1549675782, 1117.3292056910147), (1549682774, 1118.8597725723334), (1549680354, 1118.8780890290184), (1549683827, 1119.2518437220297), (1549681046, 1120.45433460368)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Max_depth\n",
    "\n",
    "### fixing random_state, change max_depth: 12 is enough; \n",
    "### Actually, I would use 7 considering the possible overfitting and complexity of model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_cv = [11328]\n",
    "res = []\n",
    "\n",
    "for i in range(10):\n",
    "    print('##### round {:d}#####'.format(i))\n",
    "    print('spliting data')\n",
    "    #time0 = round(time.time())\n",
    "    time0 = 1549681046 \n",
    "    x_train, x_valid, y_train, y_valid =train_test_split(X, y, test_size=0.1, random_state=time0)\n",
    "    d_train = lgb.Dataset(x_train, label=y_train, categorical_feature = categorical_columns)\n",
    "    d_valid = lgb.Dataset(x_valid, label=y_valid, categorical_feature = categorical_columns)\n",
    "\n",
    "    print('start training')\n",
    "    num_round = 10000\n",
    "    #dpth = i + 13\n",
    "    param['max_depth'] = 7\n",
    "    model = lgb.train(param, d_train, num_round, valid_sets = [d_train, d_valid], verbose_eval=500)\n",
    "    gc.collect()\n",
    "\n",
    "    print('calculating CV')\n",
    "    oof  = np.exp(model.predict(x_valid, num_iteration=model.best_iteration)) - shift\n",
    "    cv = mean_absolute_error(np.exp(y_valid)-shift, oof)\n",
    "    print(\"CV score: {:d}, {:<8.5f}\".format(dpth, cv))\n",
    "    res.append((dpth, cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#time0 = 1549681046 #( 10th lowest CV)\n",
    "\n",
    "res0 = [(3, 1140.152018451391),\n",
    " (4, 1129.1409075651584),\n",
    " (5, 1127.0458177248079),\n",
    " (6, 1124.1251423620197),\n",
    " (7, 1122.698941569006),\n",
    " (8, 1122.85474728398),\n",
    " (9, 1122.0444022389092),\n",
    " (10, 1121.4115469601754),\n",
    " (11, 1121.094762265545),\n",
    " (12, 1120.7415510162666)]\n",
    "res1 = [(13, 1120.5481205408764),\n",
    " (14, 1120.6450444199415),\n",
    " (15, 1120.433970985074),\n",
    " (16, 1120.6001638882867),\n",
    " (17, 1120.2508547644234),\n",
    " (18, 1120.667209650019),\n",
    " (19, 1120.6219726481531),\n",
    " (20, 1120.4754970603399),\n",
    " (21, 1120.2780770667464),\n",
    " (22, 1120.8271911510733)]\n",
    "\n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "x_dpth, y_cv = [], []\n",
    "for i in range(10):\n",
    "    x_dpth += [res0[i][0]]\n",
    "    y_cv += [res0[i][1]]\n",
    "for i in range(10):\n",
    "    x_dpth += [res1[i][0]]\n",
    "    y_cv += [res1[i][1]]\n",
    "plt.plot(x_dpth, y_cv)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.0005, 1123.8092241002064), (1, 0.001, 1124.7920896750043), (2, 0.002, 1122.698941569006), (3, 0.004, 1123.9724659596116), (4, 0.008, 1124.5543607670265), (5, 0.016, 1125.6144338198803), (6, 0.032, 1128.7333370298704), (7, 0.064, 1126.6428994092362)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(res)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt  \n",
    "#time0 = 1549671965 #(lowest CV)\n",
    "res0 = [(3, 1124.3061063822117), (4, 1113.0174239036055), (5, 1110.9648352067504), (6, 1105.4157118781789), (7, 1104.0016524232772), (8, 1103.1292197374542), (9, 1102.7978270009464), (10, 1102.32113105329), (11, 1101.689194571118), (12, 1101.5382428224923)]\n",
    "res1 = [(13, 1101.1599098171837),\n",
    " (14, 1101.022491828017),\n",
    " (15, 1100.9223597200016),\n",
    " (16, 1100.7920833262926),\n",
    " (17, 1101.3446330580416)]\n",
    "x_dpth, y_cv = [], []\n",
    "for i in range(10):\n",
    "    x_dpth += [res0[i][0]]\n",
    "    y_cv += [res0[i][1]]\n",
    "for i in range(5):\n",
    "    x_dpth += [res1[i][0]]\n",
    "    y_cv += [res1[i][1]]\n",
    "plt.plot(x_dpth, y_cv)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Learning_rate: 0.002 is good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### round 0#####\n",
      "spliting data\n",
      "start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macssd/anaconda3/lib/python3.6/site-packages/lightgbm/engine.py:113: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/Users/macssd/anaconda3/lib/python3.6/site-packages/lightgbm/engine.py:118: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/Users/macssd/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:1184: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "/Users/macssd/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:742: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.553745\tvalid_1's l1: 0.5496\n",
      "[1000]\ttraining's l1: 0.518592\tvalid_1's l1: 0.515006\n",
      "[1500]\ttraining's l1: 0.490199\tvalid_1's l1: 0.487297\n",
      "[2000]\ttraining's l1: 0.467361\tvalid_1's l1: 0.465224\n",
      "[2500]\ttraining's l1: 0.449575\tvalid_1's l1: 0.448276\n",
      "[3000]\ttraining's l1: 0.43506\tvalid_1's l1: 0.434703\n",
      "[3500]\ttraining's l1: 0.423718\tvalid_1's l1: 0.424308\n",
      "[4000]\ttraining's l1: 0.41442\tvalid_1's l1: 0.415897\n",
      "[4500]\ttraining's l1: 0.406861\tvalid_1's l1: 0.40922\n",
      "[5000]\ttraining's l1: 0.400688\tvalid_1's l1: 0.403879\n",
      "[5500]\ttraining's l1: 0.395528\tvalid_1's l1: 0.399509\n",
      "[6000]\ttraining's l1: 0.391304\tvalid_1's l1: 0.39603\n",
      "[6500]\ttraining's l1: 0.387695\tvalid_1's l1: 0.393123\n",
      "[7000]\ttraining's l1: 0.384607\tvalid_1's l1: 0.390653\n",
      "[7500]\ttraining's l1: 0.381913\tvalid_1's l1: 0.388545\n",
      "[8000]\ttraining's l1: 0.37964\tvalid_1's l1: 0.386849\n",
      "[8500]\ttraining's l1: 0.377653\tvalid_1's l1: 0.3854\n",
      "[9000]\ttraining's l1: 0.375893\tvalid_1's l1: 0.384176\n",
      "[9500]\ttraining's l1: 0.374314\tvalid_1's l1: 0.383094\n",
      "[10000]\ttraining's l1: 0.372919\tvalid_1's l1: 0.382166\n",
      "[10500]\ttraining's l1: 0.371658\tvalid_1's l1: 0.381374\n",
      "[11000]\ttraining's l1: 0.370522\tvalid_1's l1: 0.380683\n",
      "[11500]\ttraining's l1: 0.369458\tvalid_1's l1: 0.380062\n",
      "[12000]\ttraining's l1: 0.368491\tvalid_1's l1: 0.3795\n",
      "[12500]\ttraining's l1: 0.367607\tvalid_1's l1: 0.378994\n",
      "[13000]\ttraining's l1: 0.366791\tvalid_1's l1: 0.378569\n",
      "[13500]\ttraining's l1: 0.366042\tvalid_1's l1: 0.378187\n",
      "[14000]\ttraining's l1: 0.365309\tvalid_1's l1: 0.377841\n",
      "[14500]\ttraining's l1: 0.364627\tvalid_1's l1: 0.377515\n",
      "[15000]\ttraining's l1: 0.364008\tvalid_1's l1: 0.377231\n",
      "[15500]\ttraining's l1: 0.3634\tvalid_1's l1: 0.376963\n",
      "[16000]\ttraining's l1: 0.36282\tvalid_1's l1: 0.376718\n",
      "[16500]\ttraining's l1: 0.362266\tvalid_1's l1: 0.376491\n",
      "[17000]\ttraining's l1: 0.361738\tvalid_1's l1: 0.376289\n",
      "[17500]\ttraining's l1: 0.361243\tvalid_1's l1: 0.376084\n",
      "[18000]\ttraining's l1: 0.360791\tvalid_1's l1: 0.375924\n",
      "[18500]\ttraining's l1: 0.360329\tvalid_1's l1: 0.375753\n",
      "[19000]\ttraining's l1: 0.359851\tvalid_1's l1: 0.37557\n",
      "[19500]\ttraining's l1: 0.359409\tvalid_1's l1: 0.375415\n",
      "[20000]\ttraining's l1: 0.358982\tvalid_1's l1: 0.375282\n",
      "[20500]\ttraining's l1: 0.358554\tvalid_1's l1: 0.375145\n",
      "[21000]\ttraining's l1: 0.358154\tvalid_1's l1: 0.375018\n",
      "[21500]\ttraining's l1: 0.357784\tvalid_1's l1: 0.374919\n",
      "[22000]\ttraining's l1: 0.357371\tvalid_1's l1: 0.374789\n",
      "[22500]\ttraining's l1: 0.356985\tvalid_1's l1: 0.374665\n",
      "[23000]\ttraining's l1: 0.356654\tvalid_1's l1: 0.374589\n",
      "[23500]\ttraining's l1: 0.356304\tvalid_1's l1: 0.374501\n",
      "[24000]\ttraining's l1: 0.355961\tvalid_1's l1: 0.374426\n",
      "[24500]\ttraining's l1: 0.355601\tvalid_1's l1: 0.374333\n",
      "[25000]\ttraining's l1: 0.355292\tvalid_1's l1: 0.374257\n",
      "[25500]\ttraining's l1: 0.354961\tvalid_1's l1: 0.374181\n",
      "[26000]\ttraining's l1: 0.354655\tvalid_1's l1: 0.374118\n",
      "[26500]\ttraining's l1: 0.354326\tvalid_1's l1: 0.374037\n",
      "[27000]\ttraining's l1: 0.353991\tvalid_1's l1: 0.373971\n",
      "[27500]\ttraining's l1: 0.353661\tvalid_1's l1: 0.373889\n",
      "[28000]\ttraining's l1: 0.353325\tvalid_1's l1: 0.373821\n",
      "[28500]\ttraining's l1: 0.353011\tvalid_1's l1: 0.373766\n",
      "[29000]\ttraining's l1: 0.352699\tvalid_1's l1: 0.373705\n",
      "[29500]\ttraining's l1: 0.352401\tvalid_1's l1: 0.37365\n",
      "[30000]\ttraining's l1: 0.352097\tvalid_1's l1: 0.373594\n",
      "[30500]\ttraining's l1: 0.351755\tvalid_1's l1: 0.373517\n",
      "[31000]\ttraining's l1: 0.351458\tvalid_1's l1: 0.373458\n",
      "[31500]\ttraining's l1: 0.35118\tvalid_1's l1: 0.37341\n",
      "[32000]\ttraining's l1: 0.350892\tvalid_1's l1: 0.373359\n",
      "[32500]\ttraining's l1: 0.350607\tvalid_1's l1: 0.373316\n",
      "[33000]\ttraining's l1: 0.350322\tvalid_1's l1: 0.373267\n",
      "[33500]\ttraining's l1: 0.350042\tvalid_1's l1: 0.373212\n",
      "[34000]\ttraining's l1: 0.349744\tvalid_1's l1: 0.373159\n",
      "[34500]\ttraining's l1: 0.349482\tvalid_1's l1: 0.37311\n",
      "[35000]\ttraining's l1: 0.349196\tvalid_1's l1: 0.37306\n",
      "[35500]\ttraining's l1: 0.348927\tvalid_1's l1: 0.373023\n",
      "[36000]\ttraining's l1: 0.34866\tvalid_1's l1: 0.372979\n",
      "[36500]\ttraining's l1: 0.348397\tvalid_1's l1: 0.372942\n",
      "[37000]\ttraining's l1: 0.348115\tvalid_1's l1: 0.372902\n",
      "[37500]\ttraining's l1: 0.347855\tvalid_1's l1: 0.372864\n",
      "[38000]\ttraining's l1: 0.34758\tvalid_1's l1: 0.372817\n",
      "[38500]\ttraining's l1: 0.347313\tvalid_1's l1: 0.372774\n",
      "[39000]\ttraining's l1: 0.347044\tvalid_1's l1: 0.372743\n",
      "[39500]\ttraining's l1: 0.346773\tvalid_1's l1: 0.37271\n",
      "[40000]\ttraining's l1: 0.346503\tvalid_1's l1: 0.37268\n",
      "[40500]\ttraining's l1: 0.346241\tvalid_1's l1: 0.372662\n",
      "[41000]\ttraining's l1: 0.345981\tvalid_1's l1: 0.372638\n",
      "[41500]\ttraining's l1: 0.345724\tvalid_1's l1: 0.372612\n",
      "[42000]\ttraining's l1: 0.34548\tvalid_1's l1: 0.372593\n",
      "[42500]\ttraining's l1: 0.345215\tvalid_1's l1: 0.372556\n",
      "[43000]\ttraining's l1: 0.344951\tvalid_1's l1: 0.372523\n",
      "[43500]\ttraining's l1: 0.344689\tvalid_1's l1: 0.37249\n",
      "[44000]\ttraining's l1: 0.344419\tvalid_1's l1: 0.372451\n",
      "[44500]\ttraining's l1: 0.344181\tvalid_1's l1: 0.372426\n",
      "[45000]\ttraining's l1: 0.343919\tvalid_1's l1: 0.372406\n",
      "[45500]\ttraining's l1: 0.343671\tvalid_1's l1: 0.372375\n",
      "[46000]\ttraining's l1: 0.343412\tvalid_1's l1: 0.372345\n",
      "[46500]\ttraining's l1: 0.343155\tvalid_1's l1: 0.372322\n",
      "[47000]\ttraining's l1: 0.342912\tvalid_1's l1: 0.37229\n",
      "[47500]\ttraining's l1: 0.342678\tvalid_1's l1: 0.372271\n",
      "[48000]\ttraining's l1: 0.342437\tvalid_1's l1: 0.372249\n",
      "[48500]\ttraining's l1: 0.342194\tvalid_1's l1: 0.372217\n",
      "[49000]\ttraining's l1: 0.341951\tvalid_1's l1: 0.372187\n",
      "[49500]\ttraining's l1: 0.341725\tvalid_1's l1: 0.37217\n",
      "[50000]\ttraining's l1: 0.34149\tvalid_1's l1: 0.372143\n",
      "[50500]\ttraining's l1: 0.341262\tvalid_1's l1: 0.372119\n",
      "[51000]\ttraining's l1: 0.341022\tvalid_1's l1: 0.372098\n",
      "[51500]\ttraining's l1: 0.340787\tvalid_1's l1: 0.372073\n",
      "[52000]\ttraining's l1: 0.340551\tvalid_1's l1: 0.372053\n",
      "[52500]\ttraining's l1: 0.340307\tvalid_1's l1: 0.372031\n",
      "[53000]\ttraining's l1: 0.34006\tvalid_1's l1: 0.372012\n",
      "[53500]\ttraining's l1: 0.33983\tvalid_1's l1: 0.371991\n",
      "[54000]\ttraining's l1: 0.339602\tvalid_1's l1: 0.371971\n",
      "[54500]\ttraining's l1: 0.339369\tvalid_1's l1: 0.371951\n",
      "[55000]\ttraining's l1: 0.339142\tvalid_1's l1: 0.371934\n",
      "[55500]\ttraining's l1: 0.338906\tvalid_1's l1: 0.37192\n",
      "[56000]\ttraining's l1: 0.338672\tvalid_1's l1: 0.371901\n",
      "[56500]\ttraining's l1: 0.338436\tvalid_1's l1: 0.371884\n",
      "[57000]\ttraining's l1: 0.338215\tvalid_1's l1: 0.37187\n",
      "[57500]\ttraining's l1: 0.33797\tvalid_1's l1: 0.371855\n",
      "[58000]\ttraining's l1: 0.337724\tvalid_1's l1: 0.371835\n",
      "[58500]\ttraining's l1: 0.337505\tvalid_1's l1: 0.371826\n",
      "[59000]\ttraining's l1: 0.337278\tvalid_1's l1: 0.371806\n",
      "Early stopping, best iteration is:\n",
      "[59060]\ttraining's l1: 0.33725\tvalid_1's l1: 0.371802\n",
      "calculating CV\n",
      "CV score: 0.000500, 1123.80922\n",
      "##### round 1#####\n",
      "spliting data\n",
      "start training\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.518284\tvalid_1's l1: 0.514689\n",
      "[1000]\ttraining's l1: 0.468002\tvalid_1's l1: 0.465909\n",
      "[1500]\ttraining's l1: 0.435602\tvalid_1's l1: 0.435209\n",
      "[2000]\ttraining's l1: 0.414511\tvalid_1's l1: 0.415962\n",
      "[2500]\ttraining's l1: 0.400789\tvalid_1's l1: 0.403952\n",
      "[3000]\ttraining's l1: 0.391245\tvalid_1's l1: 0.395957\n",
      "[3500]\ttraining's l1: 0.38467\tvalid_1's l1: 0.390657\n",
      "[4000]\ttraining's l1: 0.379738\tvalid_1's l1: 0.386897\n",
      "[4500]\ttraining's l1: 0.375969\tvalid_1's l1: 0.384206\n",
      "[5000]\ttraining's l1: 0.372986\tvalid_1's l1: 0.382179\n",
      "[5500]\ttraining's l1: 0.370552\tvalid_1's l1: 0.380654\n",
      "[6000]\ttraining's l1: 0.368588\tvalid_1's l1: 0.379521\n",
      "[6500]\ttraining's l1: 0.366848\tvalid_1's l1: 0.378589\n",
      "[7000]\ttraining's l1: 0.365328\tvalid_1's l1: 0.377817\n",
      "[7500]\ttraining's l1: 0.364002\tvalid_1's l1: 0.377201\n",
      "[8000]\ttraining's l1: 0.362815\tvalid_1's l1: 0.376681\n",
      "[8500]\ttraining's l1: 0.361736\tvalid_1's l1: 0.37626\n",
      "[9000]\ttraining's l1: 0.360833\tvalid_1's l1: 0.375941\n",
      "[9500]\ttraining's l1: 0.359864\tvalid_1's l1: 0.375561\n",
      "[10000]\ttraining's l1: 0.358974\tvalid_1's l1: 0.375278\n",
      "[10500]\ttraining's l1: 0.358196\tvalid_1's l1: 0.37505\n",
      "[11000]\ttraining's l1: 0.357434\tvalid_1's l1: 0.374834\n",
      "[11500]\ttraining's l1: 0.356662\tvalid_1's l1: 0.374596\n",
      "[12000]\ttraining's l1: 0.355936\tvalid_1's l1: 0.374383\n",
      "[12500]\ttraining's l1: 0.355231\tvalid_1's l1: 0.374206\n",
      "[13000]\ttraining's l1: 0.354552\tvalid_1's l1: 0.374057\n",
      "[13500]\ttraining's l1: 0.353888\tvalid_1's l1: 0.373891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14000]\ttraining's l1: 0.353271\tvalid_1's l1: 0.373781\n",
      "[14500]\ttraining's l1: 0.352641\tvalid_1's l1: 0.37364\n",
      "[15000]\ttraining's l1: 0.352045\tvalid_1's l1: 0.373524\n",
      "[15500]\ttraining's l1: 0.351465\tvalid_1's l1: 0.373405\n",
      "[16000]\ttraining's l1: 0.35086\tvalid_1's l1: 0.373293\n",
      "[16500]\ttraining's l1: 0.350263\tvalid_1's l1: 0.37319\n",
      "[17000]\ttraining's l1: 0.349676\tvalid_1's l1: 0.373098\n",
      "[17500]\ttraining's l1: 0.349116\tvalid_1's l1: 0.372992\n",
      "[18000]\ttraining's l1: 0.348579\tvalid_1's l1: 0.372907\n",
      "[18500]\ttraining's l1: 0.348047\tvalid_1's l1: 0.372837\n",
      "[19000]\ttraining's l1: 0.347496\tvalid_1's l1: 0.372765\n",
      "[19500]\ttraining's l1: 0.346933\tvalid_1's l1: 0.372703\n",
      "[20000]\ttraining's l1: 0.346413\tvalid_1's l1: 0.372644\n",
      "[20500]\ttraining's l1: 0.345881\tvalid_1's l1: 0.372582\n",
      "[21000]\ttraining's l1: 0.345374\tvalid_1's l1: 0.37253\n",
      "[21500]\ttraining's l1: 0.344875\tvalid_1's l1: 0.372481\n",
      "[22000]\ttraining's l1: 0.344333\tvalid_1's l1: 0.372407\n",
      "[22500]\ttraining's l1: 0.343799\tvalid_1's l1: 0.37234\n",
      "[23000]\ttraining's l1: 0.343326\tvalid_1's l1: 0.372306\n",
      "Early stopping, best iteration is:\n",
      "[23175]\ttraining's l1: 0.343146\tvalid_1's l1: 0.372296\n",
      "calculating CV\n",
      "CV score: 0.001000, 1124.79209\n",
      "##### round 2#####\n",
      "spliting data\n",
      "start training\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.467684\tvalid_1's l1: 0.465541\n",
      "[1000]\ttraining's l1: 0.414975\tvalid_1's l1: 0.41645\n",
      "[1500]\ttraining's l1: 0.391452\tvalid_1's l1: 0.396107\n",
      "[2000]\ttraining's l1: 0.379691\tvalid_1's l1: 0.386827\n",
      "[2500]\ttraining's l1: 0.373008\tvalid_1's l1: 0.382187\n",
      "[3000]\ttraining's l1: 0.368583\tvalid_1's l1: 0.379522\n",
      "[3500]\ttraining's l1: 0.365537\tvalid_1's l1: 0.377945\n",
      "[4000]\ttraining's l1: 0.36296\tvalid_1's l1: 0.376768\n",
      "[4500]\ttraining's l1: 0.360901\tvalid_1's l1: 0.376019\n",
      "[5000]\ttraining's l1: 0.359098\tvalid_1's l1: 0.375331\n",
      "[5500]\ttraining's l1: 0.35747\tvalid_1's l1: 0.37482\n",
      "[6000]\ttraining's l1: 0.356043\tvalid_1's l1: 0.374464\n",
      "[6500]\ttraining's l1: 0.354668\tvalid_1's l1: 0.374139\n",
      "[7000]\ttraining's l1: 0.353365\tvalid_1's l1: 0.373814\n",
      "[7500]\ttraining's l1: 0.352129\tvalid_1's l1: 0.373561\n",
      "[8000]\ttraining's l1: 0.350954\tvalid_1's l1: 0.373345\n",
      "[8500]\ttraining's l1: 0.349764\tvalid_1's l1: 0.373122\n",
      "[9000]\ttraining's l1: 0.348754\tvalid_1's l1: 0.372997\n",
      "[9500]\ttraining's l1: 0.34762\tvalid_1's l1: 0.372782\n",
      "[10000]\ttraining's l1: 0.346532\tvalid_1's l1: 0.372643\n",
      "[10500]\ttraining's l1: 0.345524\tvalid_1's l1: 0.372523\n",
      "[11000]\ttraining's l1: 0.344467\tvalid_1's l1: 0.372409\n",
      "[11500]\ttraining's l1: 0.343464\tvalid_1's l1: 0.372305\n",
      "[12000]\ttraining's l1: 0.342479\tvalid_1's l1: 0.372185\n",
      "[12500]\ttraining's l1: 0.341509\tvalid_1's l1: 0.372081\n",
      "[13000]\ttraining's l1: 0.340561\tvalid_1's l1: 0.371993\n",
      "[13500]\ttraining's l1: 0.339654\tvalid_1's l1: 0.371894\n",
      "[14000]\ttraining's l1: 0.338748\tvalid_1's l1: 0.371837\n",
      "[14500]\ttraining's l1: 0.33783\tvalid_1's l1: 0.371782\n",
      "[15000]\ttraining's l1: 0.336951\tvalid_1's l1: 0.371723\n",
      "[15500]\ttraining's l1: 0.336058\tvalid_1's l1: 0.37167\n",
      "[16000]\ttraining's l1: 0.335163\tvalid_1's l1: 0.371606\n",
      "[16500]\ttraining's l1: 0.334272\tvalid_1's l1: 0.371568\n",
      "[17000]\ttraining's l1: 0.333371\tvalid_1's l1: 0.371538\n",
      "[17500]\ttraining's l1: 0.332506\tvalid_1's l1: 0.371492\n",
      "[18000]\ttraining's l1: 0.331644\tvalid_1's l1: 0.37146\n",
      "Early stopping, best iteration is:\n",
      "[18273]\ttraining's l1: 0.331187\tvalid_1's l1: 0.371433\n",
      "calculating CV\n",
      "CV score: 0.002000, 1122.69894\n",
      "##### round 3#####\n",
      "spliting data\n",
      "start training\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.414645\tvalid_1's l1: 0.416306\n",
      "[1000]\ttraining's l1: 0.379902\tvalid_1's l1: 0.387174\n",
      "[1500]\ttraining's l1: 0.368625\tvalid_1's l1: 0.379643\n",
      "[2000]\ttraining's l1: 0.363078\tvalid_1's l1: 0.376879\n",
      "[2500]\ttraining's l1: 0.359401\tvalid_1's l1: 0.375569\n",
      "[3000]\ttraining's l1: 0.356376\tvalid_1's l1: 0.374665\n",
      "[3500]\ttraining's l1: 0.353841\tvalid_1's l1: 0.374042\n",
      "[4000]\ttraining's l1: 0.35137\tvalid_1's l1: 0.37351\n",
      "[4500]\ttraining's l1: 0.349144\tvalid_1's l1: 0.373213\n",
      "[5000]\ttraining's l1: 0.346971\tvalid_1's l1: 0.372891\n",
      "[5500]\ttraining's l1: 0.344833\tvalid_1's l1: 0.37265\n",
      "[6000]\ttraining's l1: 0.342907\tvalid_1's l1: 0.372474\n",
      "[6500]\ttraining's l1: 0.341014\tvalid_1's l1: 0.372313\n",
      "[7000]\ttraining's l1: 0.339086\tvalid_1's l1: 0.372138\n",
      "[7500]\ttraining's l1: 0.337305\tvalid_1's l1: 0.372059\n",
      "[8000]\ttraining's l1: 0.335576\tvalid_1's l1: 0.371961\n",
      "[8500]\ttraining's l1: 0.333798\tvalid_1's l1: 0.371825\n",
      "[9000]\ttraining's l1: 0.332121\tvalid_1's l1: 0.371803\n",
      "Early stopping, best iteration is:\n",
      "[9295]\ttraining's l1: 0.331072\tvalid_1's l1: 0.371728\n",
      "calculating CV\n",
      "CV score: 0.004000, 1123.97247\n",
      "##### round 4#####\n",
      "spliting data\n",
      "start training\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.379761\tvalid_1's l1: 0.387148\n",
      "[1000]\ttraining's l1: 0.363175\tvalid_1's l1: 0.377072\n",
      "[1500]\ttraining's l1: 0.356269\tvalid_1's l1: 0.374569\n",
      "[2000]\ttraining's l1: 0.351119\tvalid_1's l1: 0.373463\n",
      "[2500]\ttraining's l1: 0.347046\tvalid_1's l1: 0.37297\n",
      "[3000]\ttraining's l1: 0.342994\tvalid_1's l1: 0.372528\n",
      "[3500]\ttraining's l1: 0.339367\tvalid_1's l1: 0.37223\n",
      "[4000]\ttraining's l1: 0.335704\tvalid_1's l1: 0.37192\n",
      "Early stopping, best iteration is:\n",
      "[4236]\ttraining's l1: 0.334022\tvalid_1's l1: 0.371835\n",
      "calculating CV\n",
      "CV score: 0.008000, 1124.55436\n",
      "##### round 5#####\n",
      "spliting data\n",
      "start training\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.363678\tvalid_1's l1: 0.377408\n",
      "[1000]\ttraining's l1: 0.351775\tvalid_1's l1: 0.373906\n",
      "[1500]\ttraining's l1: 0.343241\tvalid_1's l1: 0.372691\n",
      "[2000]\ttraining's l1: 0.335844\tvalid_1's l1: 0.372236\n",
      "Early stopping, best iteration is:\n",
      "[2196]\ttraining's l1: 0.333282\tvalid_1's l1: 0.372076\n",
      "calculating CV\n",
      "CV score: 0.016000, 1125.61443\n",
      "##### round 6#####\n",
      "spliting data\n",
      "start training\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.352218\tvalid_1's l1: 0.374768\n",
      "[1000]\ttraining's l1: 0.337046\tvalid_1's l1: 0.373182\n",
      "[1500]\ttraining's l1: 0.324438\tvalid_1's l1: 0.372724\n",
      "Early stopping, best iteration is:\n",
      "[1450]\ttraining's l1: 0.325395\tvalid_1's l1: 0.372701\n",
      "calculating CV\n",
      "CV score: 0.032000, 1128.73334\n",
      "##### round 7#####\n",
      "spliting data\n",
      "start training\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.337637\tvalid_1's l1: 0.373146\n",
      "Early stopping, best iteration is:\n",
      "[703]\ttraining's l1: 0.327838\tvalid_1's l1: 0.372734\n",
      "calculating CV\n",
      "CV score: 0.064000, 1126.64290\n"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "\n",
    "for i in range(8):\n",
    "    print('##### round {:d}#####'.format(i))\n",
    "    print('spliting data')\n",
    "    #time0 = round(time.time())\n",
    "    #time0 = 1549671965 \n",
    "    time0 = 1549681046\n",
    "    x_train, x_valid, y_train, y_valid =train_test_split(X, y, test_size=0.1, random_state=time0)\n",
    "    d_train = lgb.Dataset(x_train, label=y_train, categorical_feature = categorical_columns)\n",
    "    d_valid = lgb.Dataset(x_valid, label=y_valid, categorical_feature = categorical_columns)\n",
    "\n",
    "    print('start training')\n",
    "    num_round = 10000\n",
    "    learning_rate = 0.0005 * (2**i)\n",
    "    param['num_iterations'] = 100000\n",
    "    param['max_depth'] = 7\n",
    "    param['learning_rate'] = learning_rate\n",
    "    model = lgb.train(param, d_train, num_round, valid_sets = [d_train, d_valid], verbose_eval=500)\n",
    "    gc.collect()\n",
    "\n",
    "    print('calculating CV')\n",
    "    oof  = np.exp(model.predict(x_valid, num_iteration=model.best_iteration)) - shift\n",
    "    cv = mean_absolute_error(np.exp(y_valid)-shift, oof)\n",
    "    print(\"CV score: {:f}, {:<8.5f}\".format(learning_rate, cv))\n",
    "    res.append((i, learning_rate, cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.0005, 1123.8092241002064), (1, 0.001, 1124.7920896750043), (2, 0.002, 1122.698941569006), (3, 0.004, 1123.9724659596116), (4, 0.008, 1124.5543607670265), (5, 0.016, 1125.6144338198803), (6, 0.032, 1128.7333370298704), (7, 0.064, 1126.6428994092362)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(res)\n",
    "\n",
    "x_lr, y_cv = [], []\n",
    "for i in range(8):\n",
    "    x_lr += [res[i][0]]\n",
    "    y_cv += [res[i][2]]\n",
    "plt.plot(x_lr, y_cv)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Grid_Param for LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'num_leaves': 200,\n",
    "     'min_data_in_leaf': 9,\n",
    "     'num_iterations': 50000,\n",
    "     'num_thread': 4,\n",
    "     'early_stopping_round': 200,\n",
    "     'objective':'regression', # notice: the default value is regression\n",
    "     'max_depth': 7,\n",
    "     'learning_rate': 0.002,\n",
    "     \"boosting\": \"gbdt\",\n",
    "     \"feature_fraction\": 0.3149,\n",
    "     \"bagging_freq\": 100,\n",
    "     \"bagging_fraction\": 0.8 ,\n",
    "     \"bagging_seed\": 2019,\n",
    "     \"metric\": 'l1',\n",
    "     \"lambda_l1\": 0.1,\n",
    "     \"random_state\": 2019,\n",
    "     \"verbosity\": -1         \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# params to search\n",
    "gridParams = {\n",
    "    'num_leaves': [10, 25, 50, 100, 200],\n",
    "    'feature_fraction': [0.3, 0.5, 0.8], # same as 'colsample_bytree'\n",
    "    'subsample': [0.7, 0.75, 0.8],\n",
    "    'reg_alpha': [1, 1.2],\n",
    "    'reg_lambda': [1,1.2, 1.4]\n",
    "}\n",
    "\n",
    "# classifier\n",
    "mdl = lgb.LGBMRegressor(min_data_in_leaf=9,\n",
    "     #num_iterations=10000,\n",
    "     num_thread=4,\n",
    "     #early_stopping_round=200,\n",
    "     objective='regression', # notice: the default value is regression\n",
    "     max_depth=7,\n",
    "     learning_rate=0.002,\n",
    "     boosting=\"gbdt\",\n",
    "     bagging_freq=100,\n",
    "     bagging_fraction=0.8 ,\n",
    "     bagging_seed=2019,\n",
    "     metric='l1',\n",
    "     lambda_l1= 0.1,\n",
    "     random_state= 2019,\n",
    "     verbosity= -1  \n",
    ")\n",
    "\n",
    "# to view the default params\n",
    "mdl.get_params().keys()\n",
    "\n",
    "# create the grid\n",
    "grid = GridSearchCV(mdl, gridParams, verbose=0, cv=4, n_jobs=4) # n_jobs: number of threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)\n",
    "\n",
    "#{'feature_fraction': 0.8, 'num_leaves': 200, 'reg_alpha': 1, 'reg_lambda': 1, 'subsample': 0.7} \n",
    "#0.14225891507699065"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'num_leaves': 200,\n",
    "     'min_data_in_leaf': 9,\n",
    "     'num_iterations': 50000,\n",
    "     'num_thread': 4,\n",
    "     'early_stopping_round': 200,\n",
    "     'objective':'regression', # notice: the default value is regression\n",
    "     'max_depth': 7,\n",
    "     'learning_rate': 0.002,\n",
    "     \"boosting\": \"gbdt\",\n",
    "     \"feature_fraction\": 0.8, # new best value\n",
    "     \"bagging_freq\": 100,\n",
    "     \"bagging_fraction\": 0.8 ,\n",
    "     \"bagging_seed\": 2019,\n",
    "     \"metric\": 'l1',\n",
    "     \"lambda_l1\": 0.1,\n",
    "     \"random_state\": 2019,\n",
    "     \"verbosity\": -1         \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### round 0#####\n",
      "spliting data\n",
      "start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macssd/anaconda3/lib/python3.6/site-packages/lightgbm/engine.py:113: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/Users/macssd/anaconda3/lib/python3.6/site-packages/lightgbm/engine.py:118: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/Users/macssd/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:1184: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "/Users/macssd/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:742: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.450968\tvalid_1's l1: 0.449823\n",
      "[1000]\ttraining's l1: 0.405446\tvalid_1's l1: 0.405324\n",
      "[1500]\ttraining's l1: 0.386396\tvalid_1's l1: 0.389259\n",
      "[2000]\ttraining's l1: 0.37584\tvalid_1's l1: 0.381734\n",
      "[2500]\ttraining's l1: 0.36914\tvalid_1's l1: 0.377682\n",
      "[3000]\ttraining's l1: 0.364318\tvalid_1's l1: 0.375236\n",
      "[3500]\ttraining's l1: 0.360754\tvalid_1's l1: 0.373843\n",
      "[4000]\ttraining's l1: 0.357887\tvalid_1's l1: 0.372808\n",
      "[4500]\ttraining's l1: 0.355485\tvalid_1's l1: 0.371998\n",
      "[5000]\ttraining's l1: 0.353235\tvalid_1's l1: 0.371479\n",
      "[5500]\ttraining's l1: 0.351354\tvalid_1's l1: 0.371075\n",
      "[6000]\ttraining's l1: 0.349629\tvalid_1's l1: 0.370772\n",
      "[6500]\ttraining's l1: 0.347962\tvalid_1's l1: 0.370557\n",
      "[7000]\ttraining's l1: 0.346442\tvalid_1's l1: 0.370333\n",
      "[7500]\ttraining's l1: 0.344947\tvalid_1's l1: 0.370181\n",
      "[8000]\ttraining's l1: 0.343476\tvalid_1's l1: 0.370006\n",
      "[8500]\ttraining's l1: 0.341974\tvalid_1's l1: 0.369888\n",
      "[9000]\ttraining's l1: 0.34053\tvalid_1's l1: 0.369743\n",
      "[9500]\ttraining's l1: 0.339152\tvalid_1's l1: 0.369605\n",
      "Early stopping, best iteration is:\n",
      "[9601]\ttraining's l1: 0.338868\tvalid_1's l1: 0.369589\n",
      "calculating CV\n",
      "CV score: 18, 1135.85751\n",
      "start predicting\n",
      "##### round 1#####\n",
      "spliting data\n",
      "start training\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.450516\tvalid_1's l1: 0.451317\n",
      "[1000]\ttraining's l1: 0.404555\tvalid_1's l1: 0.409304\n",
      "[1500]\ttraining's l1: 0.385623\tvalid_1's l1: 0.393555\n",
      "[2000]\ttraining's l1: 0.3751\tvalid_1's l1: 0.385583\n",
      "[2500]\ttraining's l1: 0.368466\tvalid_1's l1: 0.381171\n",
      "[3000]\ttraining's l1: 0.363719\tvalid_1's l1: 0.378649\n",
      "[3500]\ttraining's l1: 0.360257\tvalid_1's l1: 0.377044\n",
      "[4000]\ttraining's l1: 0.357769\tvalid_1's l1: 0.376126\n",
      "[4500]\ttraining's l1: 0.355214\tvalid_1's l1: 0.375273\n",
      "[5000]\ttraining's l1: 0.352959\tvalid_1's l1: 0.374922\n",
      "[5500]\ttraining's l1: 0.350936\tvalid_1's l1: 0.374458\n",
      "[6000]\ttraining's l1: 0.349163\tvalid_1's l1: 0.374127\n",
      "[6500]\ttraining's l1: 0.347437\tvalid_1's l1: 0.373884\n",
      "[7000]\ttraining's l1: 0.345799\tvalid_1's l1: 0.373652\n",
      "[7500]\ttraining's l1: 0.344303\tvalid_1's l1: 0.37355\n",
      "[8000]\ttraining's l1: 0.342769\tvalid_1's l1: 0.373383\n",
      "[8500]\ttraining's l1: 0.34165\tvalid_1's l1: 0.373362\n",
      "Early stopping, best iteration is:\n",
      "[8301]\ttraining's l1: 0.342121\tvalid_1's l1: 0.373359\n",
      "calculating CV\n",
      "CV score: 18, 1149.49660\n",
      "start predicting\n",
      "##### round 2#####\n",
      "spliting data\n",
      "start training\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.450483\tvalid_1's l1: 0.452855\n",
      "[1000]\ttraining's l1: 0.404767\tvalid_1's l1: 0.410192\n",
      "[1500]\ttraining's l1: 0.385999\tvalid_1's l1: 0.394165\n",
      "[2000]\ttraining's l1: 0.375471\tvalid_1's l1: 0.386084\n",
      "[2500]\ttraining's l1: 0.36864\tvalid_1's l1: 0.381677\n",
      "[3000]\ttraining's l1: 0.363931\tvalid_1's l1: 0.378993\n",
      "[3500]\ttraining's l1: 0.360621\tvalid_1's l1: 0.377497\n",
      "[4000]\ttraining's l1: 0.357825\tvalid_1's l1: 0.376378\n",
      "[4500]\ttraining's l1: 0.35535\tvalid_1's l1: 0.375529\n",
      "[5000]\ttraining's l1: 0.353217\tvalid_1's l1: 0.374907\n",
      "[5500]\ttraining's l1: 0.35114\tvalid_1's l1: 0.374439\n",
      "[6000]\ttraining's l1: 0.349375\tvalid_1's l1: 0.374116\n",
      "[6500]\ttraining's l1: 0.347596\tvalid_1's l1: 0.373779\n",
      "[7000]\ttraining's l1: 0.346024\tvalid_1's l1: 0.373523\n",
      "[7500]\ttraining's l1: 0.344588\tvalid_1's l1: 0.373338\n",
      "[8000]\ttraining's l1: 0.343171\tvalid_1's l1: 0.373221\n",
      "[8500]\ttraining's l1: 0.341577\tvalid_1's l1: 0.37308\n",
      "[9000]\ttraining's l1: 0.34008\tvalid_1's l1: 0.372922\n",
      "Early stopping, best iteration is:\n",
      "[9290]\ttraining's l1: 0.339314\tvalid_1's l1: 0.372851\n",
      "calculating CV\n",
      "CV score: 18, 1140.38821\n",
      "start predicting\n",
      "##### round 3#####\n",
      "spliting data\n",
      "start training\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.450133\tvalid_1's l1: 0.454244\n",
      "[1000]\ttraining's l1: 0.404409\tvalid_1's l1: 0.412019\n",
      "[1500]\ttraining's l1: 0.385534\tvalid_1's l1: 0.396519\n",
      "[2000]\ttraining's l1: 0.375013\tvalid_1's l1: 0.388625\n",
      "[2500]\ttraining's l1: 0.368302\tvalid_1's l1: 0.384262\n",
      "[3000]\ttraining's l1: 0.363463\tvalid_1's l1: 0.381442\n",
      "[3500]\ttraining's l1: 0.360007\tvalid_1's l1: 0.38001\n",
      "[4000]\ttraining's l1: 0.357355\tvalid_1's l1: 0.379062\n",
      "[4500]\ttraining's l1: 0.354976\tvalid_1's l1: 0.378267\n",
      "[5000]\ttraining's l1: 0.352714\tvalid_1's l1: 0.377635\n",
      "[5500]\ttraining's l1: 0.350742\tvalid_1's l1: 0.377204\n",
      "[6000]\ttraining's l1: 0.349008\tvalid_1's l1: 0.376893\n",
      "[6500]\ttraining's l1: 0.347155\tvalid_1's l1: 0.376541\n",
      "[7000]\ttraining's l1: 0.34544\tvalid_1's l1: 0.376273\n",
      "[7500]\ttraining's l1: 0.344058\tvalid_1's l1: 0.37616\n",
      "[8000]\ttraining's l1: 0.342581\tvalid_1's l1: 0.376012\n",
      "[8500]\ttraining's l1: 0.341102\tvalid_1's l1: 0.375807\n",
      "[9000]\ttraining's l1: 0.339754\tvalid_1's l1: 0.375777\n",
      "Early stopping, best iteration is:\n",
      "[8818]\ttraining's l1: 0.340242\tvalid_1's l1: 0.375771\n",
      "calculating CV\n",
      "CV score: 18, 1143.58102\n",
      "start predicting\n",
      "##### round 4#####\n",
      "spliting data\n",
      "start training\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.450502\tvalid_1's l1: 0.45203\n",
      "[1000]\ttraining's l1: 0.404723\tvalid_1's l1: 0.40904\n",
      "[1500]\ttraining's l1: 0.385923\tvalid_1's l1: 0.393145\n",
      "[2000]\ttraining's l1: 0.375312\tvalid_1's l1: 0.385193\n",
      "[2500]\ttraining's l1: 0.368446\tvalid_1's l1: 0.380806\n",
      "[3000]\ttraining's l1: 0.363684\tvalid_1's l1: 0.378446\n",
      "[3500]\ttraining's l1: 0.360399\tvalid_1's l1: 0.377008\n",
      "[4000]\ttraining's l1: 0.357463\tvalid_1's l1: 0.376039\n",
      "[4500]\ttraining's l1: 0.355084\tvalid_1's l1: 0.375387\n",
      "[5000]\ttraining's l1: 0.352845\tvalid_1's l1: 0.374894\n",
      "[5500]\ttraining's l1: 0.350897\tvalid_1's l1: 0.374526\n",
      "[6000]\ttraining's l1: 0.349001\tvalid_1's l1: 0.374172\n",
      "[6500]\ttraining's l1: 0.347256\tvalid_1's l1: 0.373897\n",
      "[7000]\ttraining's l1: 0.345737\tvalid_1's l1: 0.373692\n",
      "[7500]\ttraining's l1: 0.344228\tvalid_1's l1: 0.373603\n",
      "Early stopping, best iteration is:\n",
      "[7496]\ttraining's l1: 0.344238\tvalid_1's l1: 0.373602\n",
      "calculating CV\n",
      "CV score: 18, 1141.87473\n",
      "start predicting\n",
      "preparing output\n",
      "time:  1550106801_11419.0\n"
     ]
    }
   ],
   "source": [
    "prediction = 0\n",
    "\n",
    "for i in range(5):\n",
    "    print('##### round {:d}#####'.format(i))\n",
    "    print('spliting data')\n",
    "    time0 = round(time.time())\n",
    "    x_train, x_valid, y_train, y_valid =train_test_split(X, y, test_size=0.1, random_state=time0)\n",
    "    d_train = lgb.Dataset(x_train, label=y_train, categorical_feature = categorical_columns)\n",
    "    d_valid = lgb.Dataset(x_valid, label=y_valid, categorical_feature = categorical_columns)\n",
    "\n",
    "    print('start training')\n",
    "    num_round = 10000\n",
    "    model = lgb.train(param, d_train, num_round, valid_sets = [d_train, d_valid], verbose_eval=500)\n",
    "    gc.collect()\n",
    "\n",
    "    print('calculating CV')\n",
    "    oof  = np.exp(model.predict(x_valid, num_iteration=model.best_iteration)) - shift\n",
    "    cv = mean_absolute_error(np.exp(y_valid)-shift, oof)\n",
    "    print(\"CV score: {:d}, {:<8.5f}\".format(i, cv))\n",
    "    \n",
    "    #dct[time0] = cv\n",
    "    \n",
    "    print('start predicting')\n",
    "    prediction += np.exp(model.predict(X_test)) - shift\n",
    "    \n",
    "print('preparing output')\n",
    "submission = pd.DataFrame()\n",
    "submission['loss'] = prediction/5\n",
    "submission['id'] = ids\n",
    "tm = str(time0) + '_' + str(round(cv*10))\n",
    "print('time: ',tm)\n",
    "submission.to_csv('submit_'+ tm +'.csv', index=False)  # time:  1550106801_11419.0\n",
    "#submission.to_csv('submit_'+ tm +'.csv.gz', compression='gzip', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### round 0#####\n",
      "spliting data\n",
      "start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macssd/anaconda3/lib/python3.6/site-packages/lightgbm/engine.py:113: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/Users/macssd/anaconda3/lib/python3.6/site-packages/lightgbm/engine.py:118: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/Users/macssd/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:1184: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "/Users/macssd/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:742: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.450121\tvalid_1's l1: 0.454598\n",
      "[1000]\ttraining's l1: 0.404405\tvalid_1's l1: 0.412304\n",
      "[1500]\ttraining's l1: 0.385465\tvalid_1's l1: 0.395769\n",
      "[2000]\ttraining's l1: 0.374993\tvalid_1's l1: 0.387885\n",
      "[2500]\ttraining's l1: 0.368346\tvalid_1's l1: 0.383577\n",
      "[3000]\ttraining's l1: 0.363515\tvalid_1's l1: 0.381046\n",
      "[3500]\ttraining's l1: 0.359998\tvalid_1's l1: 0.3795\n",
      "[4000]\ttraining's l1: 0.357172\tvalid_1's l1: 0.378482\n",
      "[4500]\ttraining's l1: 0.354835\tvalid_1's l1: 0.377741\n",
      "[5000]\ttraining's l1: 0.352711\tvalid_1's l1: 0.377188\n",
      "[5500]\ttraining's l1: 0.350823\tvalid_1's l1: 0.376845\n",
      "[6000]\ttraining's l1: 0.348977\tvalid_1's l1: 0.376562\n",
      "[6500]\ttraining's l1: 0.347276\tvalid_1's l1: 0.376191\n",
      "[7000]\ttraining's l1: 0.345666\tvalid_1's l1: 0.376062\n",
      "[7500]\ttraining's l1: 0.344095\tvalid_1's l1: 0.375864\n",
      "[8000]\ttraining's l1: 0.34255\tvalid_1's l1: 0.375681\n",
      "[8500]\ttraining's l1: 0.341141\tvalid_1's l1: 0.375496\n",
      "Early stopping, best iteration is:\n",
      "[8703]\ttraining's l1: 0.340563\tvalid_1's l1: 0.375429\n",
      "calculating CV\n",
      "Round: 0, CV score: 1144.01068\n",
      "start predicting\n",
      "##### round 1#####\n",
      "spliting data\n",
      "start training\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.450395\tvalid_1's l1: 0.455826\n",
      "[1000]\ttraining's l1: 0.404471\tvalid_1's l1: 0.411975\n",
      "[1500]\ttraining's l1: 0.385337\tvalid_1's l1: 0.395353\n",
      "[2000]\ttraining's l1: 0.374838\tvalid_1's l1: 0.387406\n",
      "[2500]\ttraining's l1: 0.368086\tvalid_1's l1: 0.383042\n",
      "[3000]\ttraining's l1: 0.363453\tvalid_1's l1: 0.380478\n",
      "[3500]\ttraining's l1: 0.359932\tvalid_1's l1: 0.37889\n",
      "[4000]\ttraining's l1: 0.357402\tvalid_1's l1: 0.377943\n",
      "[4500]\ttraining's l1: 0.355041\tvalid_1's l1: 0.377288\n",
      "[5000]\ttraining's l1: 0.35285\tvalid_1's l1: 0.376786\n",
      "[5500]\ttraining's l1: 0.350737\tvalid_1's l1: 0.376454\n",
      "[6000]\ttraining's l1: 0.348767\tvalid_1's l1: 0.376167\n",
      "[6500]\ttraining's l1: 0.346978\tvalid_1's l1: 0.375922\n",
      "[7000]\ttraining's l1: 0.345565\tvalid_1's l1: 0.375784\n",
      "[7500]\ttraining's l1: 0.343992\tvalid_1's l1: 0.375668\n",
      "[8000]\ttraining's l1: 0.342506\tvalid_1's l1: 0.375565\n",
      "[8500]\ttraining's l1: 0.341038\tvalid_1's l1: 0.375441\n",
      "[9000]\ttraining's l1: 0.339673\tvalid_1's l1: 0.375283\n",
      "Early stopping, best iteration is:\n",
      "[9099]\ttraining's l1: 0.339438\tvalid_1's l1: 0.375261\n",
      "calculating CV\n",
      "Round: 1, CV score: 1144.19263\n",
      "start predicting\n",
      "##### round 2#####\n",
      "spliting data\n",
      "start training\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.450514\tvalid_1's l1: 0.451661\n",
      "[1000]\ttraining's l1: 0.404572\tvalid_1's l1: 0.409497\n",
      "[1500]\ttraining's l1: 0.385849\tvalid_1's l1: 0.393661\n",
      "[2000]\ttraining's l1: 0.375062\tvalid_1's l1: 0.385605\n",
      "[2500]\ttraining's l1: 0.368627\tvalid_1's l1: 0.381579\n",
      "[3000]\ttraining's l1: 0.363739\tvalid_1's l1: 0.378915\n",
      "[3500]\ttraining's l1: 0.360363\tvalid_1's l1: 0.37744\n",
      "[4000]\ttraining's l1: 0.357404\tvalid_1's l1: 0.376367\n",
      "[4500]\ttraining's l1: 0.354841\tvalid_1's l1: 0.37564\n",
      "[5000]\ttraining's l1: 0.352694\tvalid_1's l1: 0.375067\n",
      "[5500]\ttraining's l1: 0.350715\tvalid_1's l1: 0.374572\n",
      "[6000]\ttraining's l1: 0.348911\tvalid_1's l1: 0.374249\n",
      "[6500]\ttraining's l1: 0.347273\tvalid_1's l1: 0.373974\n",
      "[7000]\ttraining's l1: 0.345657\tvalid_1's l1: 0.37366\n",
      "[7500]\ttraining's l1: 0.34419\tvalid_1's l1: 0.373475\n",
      "[8000]\ttraining's l1: 0.342805\tvalid_1's l1: 0.373267\n",
      "[8500]\ttraining's l1: 0.341489\tvalid_1's l1: 0.373112\n",
      "[9000]\ttraining's l1: 0.340085\tvalid_1's l1: 0.372996\n",
      "[9500]\ttraining's l1: 0.338732\tvalid_1's l1: 0.372943\n",
      "[10000]\ttraining's l1: 0.337431\tvalid_1's l1: 0.372898\n",
      "Early stopping, best iteration is:\n",
      "[10100]\ttraining's l1: 0.337191\tvalid_1's l1: 0.372868\n",
      "calculating CV\n",
      "Round: 2, CV score: 1142.05729\n",
      "start predicting\n",
      "##### round 3#####\n",
      "spliting data\n",
      "start training\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.450349\tvalid_1's l1: 0.4545\n",
      "[1000]\ttraining's l1: 0.404817\tvalid_1's l1: 0.410035\n",
      "[1500]\ttraining's l1: 0.385929\tvalid_1's l1: 0.39342\n",
      "[2000]\ttraining's l1: 0.375327\tvalid_1's l1: 0.38528\n",
      "[2500]\ttraining's l1: 0.368544\tvalid_1's l1: 0.380795\n",
      "[3000]\ttraining's l1: 0.363847\tvalid_1's l1: 0.37831\n",
      "[3500]\ttraining's l1: 0.360246\tvalid_1's l1: 0.376822\n",
      "[4000]\ttraining's l1: 0.357621\tvalid_1's l1: 0.375919\n",
      "[4500]\ttraining's l1: 0.355234\tvalid_1's l1: 0.375175\n",
      "[5000]\ttraining's l1: 0.352901\tvalid_1's l1: 0.374668\n",
      "[5500]\ttraining's l1: 0.35088\tvalid_1's l1: 0.374235\n",
      "[6000]\ttraining's l1: 0.348826\tvalid_1's l1: 0.373872\n",
      "[6500]\ttraining's l1: 0.347053\tvalid_1's l1: 0.373735\n",
      "[7000]\ttraining's l1: 0.345408\tvalid_1's l1: 0.373569\n",
      "[7500]\ttraining's l1: 0.343939\tvalid_1's l1: 0.373403\n",
      "Early stopping, best iteration is:\n",
      "[7614]\ttraining's l1: 0.343593\tvalid_1's l1: 0.373377\n",
      "calculating CV\n",
      "Round: 3, CV score: 1156.73811\n",
      "start predicting\n",
      "##### round 4#####\n",
      "spliting data\n",
      "start training\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.45054\tvalid_1's l1: 0.450773\n",
      "[1000]\ttraining's l1: 0.404818\tvalid_1's l1: 0.408211\n",
      "[1500]\ttraining's l1: 0.385783\tvalid_1's l1: 0.392198\n",
      "[2000]\ttraining's l1: 0.375516\tvalid_1's l1: 0.384339\n",
      "[2500]\ttraining's l1: 0.368762\tvalid_1's l1: 0.379807\n",
      "[3000]\ttraining's l1: 0.363986\tvalid_1's l1: 0.377117\n",
      "[3500]\ttraining's l1: 0.36028\tvalid_1's l1: 0.375611\n",
      "[4000]\ttraining's l1: 0.357396\tvalid_1's l1: 0.374599\n",
      "[4500]\ttraining's l1: 0.355266\tvalid_1's l1: 0.374005\n",
      "[5000]\ttraining's l1: 0.352843\tvalid_1's l1: 0.373379\n",
      "[5500]\ttraining's l1: 0.351047\tvalid_1's l1: 0.37289\n",
      "[6000]\ttraining's l1: 0.349398\tvalid_1's l1: 0.372548\n",
      "[6500]\ttraining's l1: 0.347645\tvalid_1's l1: 0.372286\n",
      "[7000]\ttraining's l1: 0.346098\tvalid_1's l1: 0.372147\n",
      "[7500]\ttraining's l1: 0.344468\tvalid_1's l1: 0.371994\n",
      "[8000]\ttraining's l1: 0.342979\tvalid_1's l1: 0.371855\n",
      "[8500]\ttraining's l1: 0.34144\tvalid_1's l1: 0.371729\n",
      "[9000]\ttraining's l1: 0.340019\tvalid_1's l1: 0.371653\n",
      "Early stopping, best iteration is:\n",
      "[9098]\ttraining's l1: 0.339713\tvalid_1's l1: 0.371598\n",
      "calculating CV\n",
      "Round: 4, CV score: 1140.25303\n",
      "start predicting\n",
      "##### round 5#####\n",
      "spliting data\n",
      "start training\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.450139\tvalid_1's l1: 0.454319\n",
      "[1000]\ttraining's l1: 0.404577\tvalid_1's l1: 0.411962\n",
      "[1500]\ttraining's l1: 0.385553\tvalid_1's l1: 0.395608\n",
      "[2000]\ttraining's l1: 0.375024\tvalid_1's l1: 0.387385\n",
      "[2500]\ttraining's l1: 0.368369\tvalid_1's l1: 0.382934\n",
      "[3000]\ttraining's l1: 0.363698\tvalid_1's l1: 0.380281\n",
      "[3500]\ttraining's l1: 0.360248\tvalid_1's l1: 0.378724\n",
      "[4000]\ttraining's l1: 0.357339\tvalid_1's l1: 0.377645\n",
      "[4500]\ttraining's l1: 0.354989\tvalid_1's l1: 0.376883\n",
      "[5000]\ttraining's l1: 0.352883\tvalid_1's l1: 0.376311\n",
      "[5500]\ttraining's l1: 0.350926\tvalid_1's l1: 0.375915\n",
      "[6000]\ttraining's l1: 0.349135\tvalid_1's l1: 0.375486\n",
      "[6500]\ttraining's l1: 0.347534\tvalid_1's l1: 0.375262\n",
      "[7000]\ttraining's l1: 0.345965\tvalid_1's l1: 0.375019\n",
      "[7500]\ttraining's l1: 0.344451\tvalid_1's l1: 0.374744\n",
      "[8000]\ttraining's l1: 0.343037\tvalid_1's l1: 0.374616\n",
      "[8500]\ttraining's l1: 0.341467\tvalid_1's l1: 0.374439\n",
      "[9000]\ttraining's l1: 0.340165\tvalid_1's l1: 0.374318\n",
      "Early stopping, best iteration is:\n",
      "[9001]\ttraining's l1: 0.340162\tvalid_1's l1: 0.374318\n",
      "calculating CV\n",
      "Round: 5, CV score: 1145.70510\n",
      "start predicting\n",
      "##### round 6#####\n",
      "spliting data\n",
      "start training\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.450621\tvalid_1's l1: 0.453226\n",
      "[1000]\ttraining's l1: 0.404655\tvalid_1's l1: 0.41103\n",
      "[1500]\ttraining's l1: 0.385679\tvalid_1's l1: 0.395197\n",
      "[2000]\ttraining's l1: 0.375279\tvalid_1's l1: 0.38738\n",
      "[2500]\ttraining's l1: 0.368297\tvalid_1's l1: 0.382933\n",
      "[3000]\ttraining's l1: 0.363708\tvalid_1's l1: 0.380517\n",
      "[3500]\ttraining's l1: 0.360121\tvalid_1's l1: 0.37892\n",
      "[4000]\ttraining's l1: 0.357379\tvalid_1's l1: 0.377885\n",
      "[4500]\ttraining's l1: 0.355016\tvalid_1's l1: 0.377136\n",
      "[5000]\ttraining's l1: 0.35263\tvalid_1's l1: 0.376497\n",
      "[5500]\ttraining's l1: 0.350611\tvalid_1's l1: 0.376178\n",
      "[6000]\ttraining's l1: 0.348937\tvalid_1's l1: 0.375874\n",
      "[6500]\ttraining's l1: 0.347301\tvalid_1's l1: 0.375599\n",
      "[7000]\ttraining's l1: 0.345849\tvalid_1's l1: 0.375422\n",
      "[7500]\ttraining's l1: 0.344332\tvalid_1's l1: 0.375257\n",
      "[8000]\ttraining's l1: 0.342767\tvalid_1's l1: 0.375155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8500]\ttraining's l1: 0.341178\tvalid_1's l1: 0.375041\n",
      "Early stopping, best iteration is:\n",
      "[8799]\ttraining's l1: 0.340417\tvalid_1's l1: 0.374958\n",
      "calculating CV\n",
      "Round: 6, CV score: 1143.39074\n",
      "start predicting\n",
      "##### round 7#####\n",
      "spliting data\n",
      "start training\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.450372\tvalid_1's l1: 0.452702\n",
      "[1000]\ttraining's l1: 0.404403\tvalid_1's l1: 0.410877\n",
      "[1500]\ttraining's l1: 0.385407\tvalid_1's l1: 0.395131\n",
      "[2000]\ttraining's l1: 0.374803\tvalid_1's l1: 0.387394\n",
      "[2500]\ttraining's l1: 0.368483\tvalid_1's l1: 0.383613\n",
      "[3000]\ttraining's l1: 0.363615\tvalid_1's l1: 0.381179\n",
      "[3500]\ttraining's l1: 0.359777\tvalid_1's l1: 0.37963\n",
      "[4000]\ttraining's l1: 0.357144\tvalid_1's l1: 0.378856\n",
      "[4500]\ttraining's l1: 0.354565\tvalid_1's l1: 0.378167\n",
      "[5000]\ttraining's l1: 0.352605\tvalid_1's l1: 0.377805\n",
      "[5500]\ttraining's l1: 0.35063\tvalid_1's l1: 0.377458\n",
      "[6000]\ttraining's l1: 0.348737\tvalid_1's l1: 0.377148\n",
      "[6500]\ttraining's l1: 0.347119\tvalid_1's l1: 0.377019\n",
      "[7000]\ttraining's l1: 0.34551\tvalid_1's l1: 0.376767\n",
      "[7500]\ttraining's l1: 0.34399\tvalid_1's l1: 0.376613\n",
      "[8000]\ttraining's l1: 0.342445\tvalid_1's l1: 0.376569\n",
      "[8500]\ttraining's l1: 0.341052\tvalid_1's l1: 0.376458\n",
      "[9000]\ttraining's l1: 0.339614\tvalid_1's l1: 0.376361\n",
      "[9500]\ttraining's l1: 0.338392\tvalid_1's l1: 0.376267\n",
      "Early stopping, best iteration is:\n",
      "[9796]\ttraining's l1: 0.337668\tvalid_1's l1: 0.376232\n",
      "calculating CV\n",
      "Round: 7, CV score: 1165.55782\n",
      "start predicting\n",
      "##### round 8#####\n",
      "spliting data\n",
      "start training\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.450758\tvalid_1's l1: 0.449222\n",
      "[1000]\ttraining's l1: 0.404906\tvalid_1's l1: 0.406507\n",
      "[1500]\ttraining's l1: 0.385918\tvalid_1's l1: 0.390008\n",
      "[2000]\ttraining's l1: 0.375688\tvalid_1's l1: 0.382006\n",
      "[2500]\ttraining's l1: 0.368761\tvalid_1's l1: 0.377476\n",
      "[3000]\ttraining's l1: 0.363999\tvalid_1's l1: 0.375015\n",
      "[3500]\ttraining's l1: 0.36051\tvalid_1's l1: 0.37342\n",
      "[4000]\ttraining's l1: 0.35777\tvalid_1's l1: 0.372348\n",
      "[4500]\ttraining's l1: 0.355168\tvalid_1's l1: 0.37163\n",
      "[5000]\ttraining's l1: 0.353009\tvalid_1's l1: 0.371084\n",
      "[5500]\ttraining's l1: 0.35108\tvalid_1's l1: 0.37068\n",
      "[6000]\ttraining's l1: 0.349337\tvalid_1's l1: 0.37028\n",
      "[6500]\ttraining's l1: 0.347601\tvalid_1's l1: 0.370058\n",
      "[7000]\ttraining's l1: 0.345951\tvalid_1's l1: 0.36978\n",
      "[7500]\ttraining's l1: 0.344442\tvalid_1's l1: 0.369533\n",
      "[8000]\ttraining's l1: 0.342853\tvalid_1's l1: 0.36929\n",
      "[8500]\ttraining's l1: 0.341601\tvalid_1's l1: 0.369145\n",
      "[9000]\ttraining's l1: 0.340268\tvalid_1's l1: 0.369098\n",
      "Early stopping, best iteration is:\n",
      "[8800]\ttraining's l1: 0.340763\tvalid_1's l1: 0.369073\n",
      "calculating CV\n",
      "Round: 8, CV score: 1117.05725\n",
      "start predicting\n",
      "##### round 9#####\n",
      "spliting data\n",
      "start training\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.450105\tvalid_1's l1: 0.4571\n",
      "[1000]\ttraining's l1: 0.404105\tvalid_1's l1: 0.414804\n",
      "[1500]\ttraining's l1: 0.38538\tvalid_1's l1: 0.398361\n",
      "[2000]\ttraining's l1: 0.374914\tvalid_1's l1: 0.389873\n",
      "[2500]\ttraining's l1: 0.368268\tvalid_1's l1: 0.385245\n",
      "[3000]\ttraining's l1: 0.363494\tvalid_1's l1: 0.382496\n",
      "[3500]\ttraining's l1: 0.359879\tvalid_1's l1: 0.380835\n",
      "[4000]\ttraining's l1: 0.35701\tvalid_1's l1: 0.379773\n",
      "[4500]\ttraining's l1: 0.354609\tvalid_1's l1: 0.379056\n",
      "[5000]\ttraining's l1: 0.352662\tvalid_1's l1: 0.378499\n",
      "[5500]\ttraining's l1: 0.35068\tvalid_1's l1: 0.378052\n",
      "[6000]\ttraining's l1: 0.348814\tvalid_1's l1: 0.37768\n",
      "[6500]\ttraining's l1: 0.347162\tvalid_1's l1: 0.377342\n",
      "[7000]\ttraining's l1: 0.345604\tvalid_1's l1: 0.377106\n",
      "[7500]\ttraining's l1: 0.34394\tvalid_1's l1: 0.376903\n",
      "[8000]\ttraining's l1: 0.342434\tvalid_1's l1: 0.376719\n",
      "[8500]\ttraining's l1: 0.341114\tvalid_1's l1: 0.376595\n",
      "Early stopping, best iteration is:\n",
      "[8574]\ttraining's l1: 0.340919\tvalid_1's l1: 0.376584\n",
      "calculating CV\n",
      "Round: 9, CV score: 1155.77676\n",
      "start predicting\n",
      "cv scores of 10 rounds:\n",
      "[1144.0106842623654, 1144.1926300113978, 1142.0572855634732, 1156.7381058895803, 1140.2530286995382, 1145.7051005758249, 1143.3907438116294, 1165.5578182560987, 1117.0572494597684, 1155.776761577437]\n",
      "preparing output\n",
      "time:  1550165473\n"
     ]
    }
   ],
   "source": [
    "prediction = 0\n",
    "lst = []\n",
    "\n",
    "rpt = 10\n",
    "for i in range(rpt):\n",
    "    print('##### round {:d}#####'.format(i))\n",
    "    print('spliting data')\n",
    "    time0 = round(time.time())\n",
    "    x_train, x_valid, y_train, y_valid =train_test_split(X, y, test_size=0.1, random_state=time0)\n",
    "    d_train = lgb.Dataset(x_train, label=y_train, categorical_feature = categorical_columns)\n",
    "    d_valid = lgb.Dataset(x_valid, label=y_valid, categorical_feature = categorical_columns)\n",
    "\n",
    "    print('start training')\n",
    "    num_round = 10000\n",
    "    param['num_thread'] = 3\n",
    "    model = lgb.train(param, d_train, num_round, valid_sets = [d_train, d_valid], verbose_eval=500)\n",
    "    gc.collect()\n",
    "\n",
    "    print('calculating CV')\n",
    "    oof  = np.exp(model.predict(x_valid, num_iteration=model.best_iteration)) - shift\n",
    "    cv = mean_absolute_error(np.exp(y_valid)-shift, oof)\n",
    "    print(\"Round: {:d}, CV score: {:<8.5f}\".format(i, cv))\n",
    "    \n",
    "    lst.append(cv)\n",
    "    \n",
    "    print('start predicting')\n",
    "    prediction += np.exp(model.predict(X_test)) - shift\n",
    "    \n",
    "print('cv scores of {:d} rounds:'.format(rpt))\n",
    "print(lst)\n",
    "print('preparing output')\n",
    "submission = pd.DataFrame()\n",
    "submission['loss'] = prediction/rpt\n",
    "submission['id'] = ids\n",
    "\n",
    "time0 = round(time.time())\n",
    "tm = str(time0) #+ '_' + str(round(cv*10))\n",
    "print('time: ',tm)\n",
    "#submission.to_csv('submit_'+ tm +'.csv', index=False)\n",
    "submission.to_csv('submit_final.csv.gz', compression='gzip', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1144.0106842623654,\n",
       " 1144.1926300113978,\n",
       " 1142.0572855634732,\n",
       " 1156.7381058895803,\n",
       " 1140.2530286995382,\n",
       " 1145.7051005758249,\n",
       " 1143.3907438116294,\n",
       " 1165.5578182560987,\n",
       " 1117.0572494597684,\n",
       " 1155.776761577437]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## retry with feature_fraction = 0.3149"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### round 0#####\n",
      "spliting data\n",
      "start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macssd/anaconda3/lib/python3.6/site-packages/lightgbm/engine.py:113: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/Users/macssd/anaconda3/lib/python3.6/site-packages/lightgbm/engine.py:118: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/Users/macssd/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:1184: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "/Users/macssd/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:742: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.467452\tvalid_1's l1: 0.466163\n",
      "[1000]\ttraining's l1: 0.415135\tvalid_1's l1: 0.414803\n",
      "[1500]\ttraining's l1: 0.39186\tvalid_1's l1: 0.393244\n",
      "[2000]\ttraining's l1: 0.380164\tvalid_1's l1: 0.383457\n",
      "[2500]\ttraining's l1: 0.373504\tvalid_1's l1: 0.378448\n",
      "[3000]\ttraining's l1: 0.369103\tvalid_1's l1: 0.375614\n",
      "[3500]\ttraining's l1: 0.365939\tvalid_1's l1: 0.373925\n",
      "[4000]\ttraining's l1: 0.363405\tvalid_1's l1: 0.372783\n",
      "[4500]\ttraining's l1: 0.361475\tvalid_1's l1: 0.371959\n",
      "[5000]\ttraining's l1: 0.359744\tvalid_1's l1: 0.371303\n",
      "[5500]\ttraining's l1: 0.358144\tvalid_1's l1: 0.370765\n",
      "[6000]\ttraining's l1: 0.356746\tvalid_1's l1: 0.370427\n",
      "[6500]\ttraining's l1: 0.355405\tvalid_1's l1: 0.370088\n",
      "[7000]\ttraining's l1: 0.35407\tvalid_1's l1: 0.369767\n",
      "[7500]\ttraining's l1: 0.352822\tvalid_1's l1: 0.369471\n",
      "[8000]\ttraining's l1: 0.351659\tvalid_1's l1: 0.369232\n",
      "[8500]\ttraining's l1: 0.350472\tvalid_1's l1: 0.369052\n",
      "[9000]\ttraining's l1: 0.349339\tvalid_1's l1: 0.368884\n",
      "[9500]\ttraining's l1: 0.348261\tvalid_1's l1: 0.36874\n",
      "[10000]\ttraining's l1: 0.347214\tvalid_1's l1: 0.368616\n",
      "[10500]\ttraining's l1: 0.346121\tvalid_1's l1: 0.368475\n",
      "[11000]\ttraining's l1: 0.345153\tvalid_1's l1: 0.368361\n",
      "[11500]\ttraining's l1: 0.344098\tvalid_1's l1: 0.368235\n",
      "[12000]\ttraining's l1: 0.343108\tvalid_1's l1: 0.368178\n",
      "[12500]\ttraining's l1: 0.342153\tvalid_1's l1: 0.36813\n",
      "[13000]\ttraining's l1: 0.341196\tvalid_1's l1: 0.368083\n",
      "[13500]\ttraining's l1: 0.340219\tvalid_1's l1: 0.368017\n",
      "[14000]\ttraining's l1: 0.339282\tvalid_1's l1: 0.36796\n",
      "[14500]\ttraining's l1: 0.338346\tvalid_1's l1: 0.367895\n",
      "[15000]\ttraining's l1: 0.337434\tvalid_1's l1: 0.367862\n",
      "Early stopping, best iteration is:\n",
      "[15239]\ttraining's l1: 0.337015\tvalid_1's l1: 0.367855\n",
      "calculating CV\n",
      "Round: 0, CV score: 1130.68739\n",
      "start predicting\n",
      "##### round 1#####\n",
      "spliting data\n",
      "start training\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.466892\tvalid_1's l1: 0.473307\n",
      "[1000]\ttraining's l1: 0.414466\tvalid_1's l1: 0.42268\n",
      "[1500]\ttraining's l1: 0.391098\tvalid_1's l1: 0.401025\n",
      "[2000]\ttraining's l1: 0.379472\tvalid_1's l1: 0.390956\n",
      "[2500]\ttraining's l1: 0.372834\tvalid_1's l1: 0.385655\n",
      "[3000]\ttraining's l1: 0.368374\tvalid_1's l1: 0.382571\n",
      "[3500]\ttraining's l1: 0.365197\tvalid_1's l1: 0.380738\n",
      "[4000]\ttraining's l1: 0.362742\tvalid_1's l1: 0.379493\n",
      "[4500]\ttraining's l1: 0.360652\tvalid_1's l1: 0.37853\n",
      "[5000]\ttraining's l1: 0.358823\tvalid_1's l1: 0.377838\n",
      "[5500]\ttraining's l1: 0.357239\tvalid_1's l1: 0.377291\n",
      "[6000]\ttraining's l1: 0.355827\tvalid_1's l1: 0.376876\n",
      "[6500]\ttraining's l1: 0.354476\tvalid_1's l1: 0.376555\n",
      "[7000]\ttraining's l1: 0.353158\tvalid_1's l1: 0.376249\n",
      "[7500]\ttraining's l1: 0.351897\tvalid_1's l1: 0.37596\n",
      "[8000]\ttraining's l1: 0.350708\tvalid_1's l1: 0.375747\n",
      "[8500]\ttraining's l1: 0.349511\tvalid_1's l1: 0.375514\n",
      "[9000]\ttraining's l1: 0.34839\tvalid_1's l1: 0.375343\n",
      "[9500]\ttraining's l1: 0.347318\tvalid_1's l1: 0.375166\n",
      "[10000]\ttraining's l1: 0.346258\tvalid_1's l1: 0.375032\n",
      "[10500]\ttraining's l1: 0.345239\tvalid_1's l1: 0.374942\n",
      "[11000]\ttraining's l1: 0.344203\tvalid_1's l1: 0.374808\n",
      "[11500]\ttraining's l1: 0.343218\tvalid_1's l1: 0.374716\n",
      "[12000]\ttraining's l1: 0.342216\tvalid_1's l1: 0.374606\n",
      "[12500]\ttraining's l1: 0.341289\tvalid_1's l1: 0.374529\n",
      "[13000]\ttraining's l1: 0.340326\tvalid_1's l1: 0.374436\n",
      "[13500]\ttraining's l1: 0.339333\tvalid_1's l1: 0.374346\n",
      "[14000]\ttraining's l1: 0.33842\tvalid_1's l1: 0.37428\n",
      "[14500]\ttraining's l1: 0.337482\tvalid_1's l1: 0.374199\n",
      "[15000]\ttraining's l1: 0.336549\tvalid_1's l1: 0.374145\n",
      "[15500]\ttraining's l1: 0.335686\tvalid_1's l1: 0.374098\n",
      "[16000]\ttraining's l1: 0.334745\tvalid_1's l1: 0.374034\n",
      "[16500]\ttraining's l1: 0.333852\tvalid_1's l1: 0.373998\n",
      "[17000]\ttraining's l1: 0.332938\tvalid_1's l1: 0.373968\n",
      "Early stopping, best iteration is:\n",
      "[16923]\ttraining's l1: 0.333079\tvalid_1's l1: 0.373963\n",
      "calculating CV\n",
      "Round: 1, CV score: 1141.57004\n",
      "start predicting\n",
      "##### round 2#####\n",
      "spliting data\n",
      "start training\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.467076\tvalid_1's l1: 0.470357\n",
      "[1000]\ttraining's l1: 0.414664\tvalid_1's l1: 0.41967\n",
      "[1500]\ttraining's l1: 0.391367\tvalid_1's l1: 0.397983\n",
      "[2000]\ttraining's l1: 0.37975\tvalid_1's l1: 0.387869\n",
      "[2500]\ttraining's l1: 0.373126\tvalid_1's l1: 0.382823\n",
      "[3000]\ttraining's l1: 0.368705\tvalid_1's l1: 0.379941\n",
      "[3500]\ttraining's l1: 0.36563\tvalid_1's l1: 0.378314\n",
      "[4000]\ttraining's l1: 0.363142\tvalid_1's l1: 0.377191\n",
      "[4500]\ttraining's l1: 0.360972\tvalid_1's l1: 0.376354\n",
      "[5000]\ttraining's l1: 0.35922\tvalid_1's l1: 0.375752\n",
      "[5500]\ttraining's l1: 0.357575\tvalid_1's l1: 0.375307\n",
      "[6000]\ttraining's l1: 0.356143\tvalid_1's l1: 0.374973\n",
      "[6500]\ttraining's l1: 0.354778\tvalid_1's l1: 0.374655\n",
      "[7000]\ttraining's l1: 0.353497\tvalid_1's l1: 0.374395\n",
      "[7500]\ttraining's l1: 0.35223\tvalid_1's l1: 0.374138\n",
      "[8000]\ttraining's l1: 0.351039\tvalid_1's l1: 0.373937\n",
      "[8500]\ttraining's l1: 0.349897\tvalid_1's l1: 0.37376\n",
      "[9000]\ttraining's l1: 0.348775\tvalid_1's l1: 0.373624\n",
      "[9500]\ttraining's l1: 0.347691\tvalid_1's l1: 0.3735\n",
      "[10000]\ttraining's l1: 0.346592\tvalid_1's l1: 0.37336\n",
      "[10500]\ttraining's l1: 0.345441\tvalid_1's l1: 0.373232\n",
      "[11000]\ttraining's l1: 0.34447\tvalid_1's l1: 0.373144\n",
      "[11500]\ttraining's l1: 0.343551\tvalid_1's l1: 0.373071\n",
      "[12000]\ttraining's l1: 0.342552\tvalid_1's l1: 0.372976\n",
      "[12500]\ttraining's l1: 0.341542\tvalid_1's l1: 0.372889\n",
      "[13000]\ttraining's l1: 0.340593\tvalid_1's l1: 0.372838\n",
      "[13500]\ttraining's l1: 0.339643\tvalid_1's l1: 0.372794\n",
      "[14000]\ttraining's l1: 0.338702\tvalid_1's l1: 0.372759\n",
      "[14500]\ttraining's l1: 0.337733\tvalid_1's l1: 0.372705\n",
      "[15000]\ttraining's l1: 0.336829\tvalid_1's l1: 0.372664\n",
      "[15500]\ttraining's l1: 0.335926\tvalid_1's l1: 0.372621\n",
      "[16000]\ttraining's l1: 0.335032\tvalid_1's l1: 0.372576\n",
      "[16500]\ttraining's l1: 0.334158\tvalid_1's l1: 0.372554\n",
      "[17000]\ttraining's l1: 0.333267\tvalid_1's l1: 0.372501\n",
      "[17500]\ttraining's l1: 0.332431\tvalid_1's l1: 0.372475\n",
      "Early stopping, best iteration is:\n",
      "[17303]\ttraining's l1: 0.332756\tvalid_1's l1: 0.37247\n",
      "calculating CV\n",
      "Round: 2, CV score: 1144.11010\n",
      "start predicting\n",
      "##### round 3#####\n",
      "spliting data\n",
      "start training\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.467252\tvalid_1's l1: 0.469732\n",
      "[1000]\ttraining's l1: 0.415008\tvalid_1's l1: 0.418603\n",
      "[1500]\ttraining's l1: 0.391696\tvalid_1's l1: 0.396655\n",
      "[2000]\ttraining's l1: 0.380016\tvalid_1's l1: 0.386355\n",
      "[2500]\ttraining's l1: 0.373297\tvalid_1's l1: 0.380971\n",
      "[3000]\ttraining's l1: 0.368891\tvalid_1's l1: 0.378\n",
      "[3500]\ttraining's l1: 0.365804\tvalid_1's l1: 0.376219\n",
      "[4000]\ttraining's l1: 0.363293\tvalid_1's l1: 0.374972\n",
      "[4500]\ttraining's l1: 0.361179\tvalid_1's l1: 0.37404\n",
      "[5000]\ttraining's l1: 0.359452\tvalid_1's l1: 0.373392\n",
      "[5500]\ttraining's l1: 0.357849\tvalid_1's l1: 0.372856\n",
      "[6000]\ttraining's l1: 0.356272\tvalid_1's l1: 0.372376\n",
      "[6500]\ttraining's l1: 0.354869\tvalid_1's l1: 0.372033\n",
      "[7000]\ttraining's l1: 0.353579\tvalid_1's l1: 0.371766\n",
      "[7500]\ttraining's l1: 0.352269\tvalid_1's l1: 0.371528\n",
      "[8000]\ttraining's l1: 0.351157\tvalid_1's l1: 0.371302\n",
      "[8500]\ttraining's l1: 0.349946\tvalid_1's l1: 0.371052\n",
      "[9000]\ttraining's l1: 0.348892\tvalid_1's l1: 0.370939\n",
      "[9500]\ttraining's l1: 0.347834\tvalid_1's l1: 0.37079\n",
      "[10000]\ttraining's l1: 0.346801\tvalid_1's l1: 0.370643\n",
      "[10500]\ttraining's l1: 0.345787\tvalid_1's l1: 0.370559\n",
      "[11000]\ttraining's l1: 0.34475\tvalid_1's l1: 0.37043\n",
      "[11500]\ttraining's l1: 0.343821\tvalid_1's l1: 0.370351\n",
      "[12000]\ttraining's l1: 0.342841\tvalid_1's l1: 0.37027\n",
      "[12500]\ttraining's l1: 0.341829\tvalid_1's l1: 0.37016\n",
      "[13000]\ttraining's l1: 0.340872\tvalid_1's l1: 0.370123\n",
      "[13500]\ttraining's l1: 0.339939\tvalid_1's l1: 0.370083\n",
      "Early stopping, best iteration is:\n",
      "[13300]\ttraining's l1: 0.340319\tvalid_1's l1: 0.370081\n",
      "calculating CV\n",
      "Round: 3, CV score: 1132.29635\n",
      "start predicting\n",
      "##### round 4#####\n",
      "spliting data\n",
      "start training\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.46719\tvalid_1's l1: 0.466589\n",
      "[1000]\ttraining's l1: 0.414773\tvalid_1's l1: 0.417007\n",
      "[1500]\ttraining's l1: 0.391374\tvalid_1's l1: 0.396401\n",
      "[2000]\ttraining's l1: 0.379682\tvalid_1's l1: 0.387002\n",
      "[2500]\ttraining's l1: 0.373014\tvalid_1's l1: 0.382273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3000]\ttraining's l1: 0.368633\tvalid_1's l1: 0.379558\n",
      "[3500]\ttraining's l1: 0.365463\tvalid_1's l1: 0.377898\n",
      "[4000]\ttraining's l1: 0.362986\tvalid_1's l1: 0.376742\n",
      "[4500]\ttraining's l1: 0.360864\tvalid_1's l1: 0.375867\n",
      "[5000]\ttraining's l1: 0.359061\tvalid_1's l1: 0.375261\n",
      "[5500]\ttraining's l1: 0.357475\tvalid_1's l1: 0.374767\n",
      "[6000]\ttraining's l1: 0.356123\tvalid_1's l1: 0.374396\n",
      "[6500]\ttraining's l1: 0.354883\tvalid_1's l1: 0.374098\n",
      "[7000]\ttraining's l1: 0.353577\tvalid_1's l1: 0.373849\n",
      "[7500]\ttraining's l1: 0.352286\tvalid_1's l1: 0.373587\n",
      "[8000]\ttraining's l1: 0.351106\tvalid_1's l1: 0.373378\n",
      "[8500]\ttraining's l1: 0.349932\tvalid_1's l1: 0.373175\n",
      "[9000]\ttraining's l1: 0.34884\tvalid_1's l1: 0.373053\n",
      "[9500]\ttraining's l1: 0.347701\tvalid_1's l1: 0.372875\n",
      "[10000]\ttraining's l1: 0.34665\tvalid_1's l1: 0.372754\n",
      "[10500]\ttraining's l1: 0.345575\tvalid_1's l1: 0.372615\n",
      "[11000]\ttraining's l1: 0.344565\tvalid_1's l1: 0.372537\n",
      "[11500]\ttraining's l1: 0.343546\tvalid_1's l1: 0.372447\n",
      "[12000]\ttraining's l1: 0.342559\tvalid_1's l1: 0.372375\n",
      "[12500]\ttraining's l1: 0.341589\tvalid_1's l1: 0.372305\n",
      "Early stopping, best iteration is:\n",
      "[12400]\ttraining's l1: 0.341757\tvalid_1's l1: 0.372302\n",
      "calculating CV\n",
      "Round: 4, CV score: 1143.13280\n",
      "start predicting\n",
      "##### round 5#####\n",
      "spliting data\n",
      "start training\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.46707\tvalid_1's l1: 0.469368\n",
      "[1000]\ttraining's l1: 0.41454\tvalid_1's l1: 0.419105\n",
      "[1500]\ttraining's l1: 0.391332\tvalid_1's l1: 0.397936\n",
      "[2000]\ttraining's l1: 0.379728\tvalid_1's l1: 0.388198\n",
      "[2500]\ttraining's l1: 0.373049\tvalid_1's l1: 0.383249\n",
      "[3000]\ttraining's l1: 0.368643\tvalid_1's l1: 0.380314\n",
      "[3500]\ttraining's l1: 0.365592\tvalid_1's l1: 0.378619\n",
      "[4000]\ttraining's l1: 0.363059\tvalid_1's l1: 0.377392\n",
      "[4500]\ttraining's l1: 0.361068\tvalid_1's l1: 0.376434\n",
      "[5000]\ttraining's l1: 0.35933\tvalid_1's l1: 0.375722\n",
      "[5500]\ttraining's l1: 0.357806\tvalid_1's l1: 0.375178\n",
      "[6000]\ttraining's l1: 0.356445\tvalid_1's l1: 0.374768\n",
      "[6500]\ttraining's l1: 0.355061\tvalid_1's l1: 0.374341\n",
      "[7000]\ttraining's l1: 0.353759\tvalid_1's l1: 0.374002\n",
      "[7500]\ttraining's l1: 0.352539\tvalid_1's l1: 0.373697\n",
      "[8000]\ttraining's l1: 0.351371\tvalid_1's l1: 0.373461\n",
      "[8500]\ttraining's l1: 0.350127\tvalid_1's l1: 0.373238\n",
      "[9000]\ttraining's l1: 0.349027\tvalid_1's l1: 0.373051\n",
      "[9500]\ttraining's l1: 0.347963\tvalid_1's l1: 0.372882\n",
      "[10000]\ttraining's l1: 0.3469\tvalid_1's l1: 0.372737\n",
      "[10500]\ttraining's l1: 0.345832\tvalid_1's l1: 0.372592\n",
      "[11000]\ttraining's l1: 0.344777\tvalid_1's l1: 0.372444\n",
      "[11500]\ttraining's l1: 0.343761\tvalid_1's l1: 0.372334\n",
      "[12000]\ttraining's l1: 0.342747\tvalid_1's l1: 0.372205\n",
      "[12500]\ttraining's l1: 0.341797\tvalid_1's l1: 0.372119\n",
      "[13000]\ttraining's l1: 0.340867\tvalid_1's l1: 0.372062\n",
      "[13500]\ttraining's l1: 0.339971\tvalid_1's l1: 0.372005\n",
      "[14000]\ttraining's l1: 0.339029\tvalid_1's l1: 0.371949\n",
      "[14500]\ttraining's l1: 0.338059\tvalid_1's l1: 0.371892\n",
      "[15000]\ttraining's l1: 0.337138\tvalid_1's l1: 0.371818\n",
      "[15500]\ttraining's l1: 0.336237\tvalid_1's l1: 0.371738\n",
      "[16000]\ttraining's l1: 0.335322\tvalid_1's l1: 0.371701\n",
      "[16500]\ttraining's l1: 0.334421\tvalid_1's l1: 0.371635\n",
      "[17000]\ttraining's l1: 0.3335\tvalid_1's l1: 0.371564\n",
      "[17500]\ttraining's l1: 0.332629\tvalid_1's l1: 0.371523\n",
      "[18000]\ttraining's l1: 0.331806\tvalid_1's l1: 0.371469\n",
      "[18500]\ttraining's l1: 0.330963\tvalid_1's l1: 0.371433\n",
      "[19000]\ttraining's l1: 0.330167\tvalid_1's l1: 0.371423\n",
      "Early stopping, best iteration is:\n",
      "[19141]\ttraining's l1: 0.329932\tvalid_1's l1: 0.371415\n",
      "calculating CV\n",
      "Round: 5, CV score: 1136.91365\n",
      "start predicting\n",
      "##### round 6#####\n",
      "spliting data\n",
      "start training\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.467724\tvalid_1's l1: 0.465056\n",
      "[1000]\ttraining's l1: 0.415026\tvalid_1's l1: 0.415816\n",
      "[1500]\ttraining's l1: 0.391541\tvalid_1's l1: 0.395154\n",
      "[2000]\ttraining's l1: 0.37988\tvalid_1's l1: 0.385586\n",
      "[2500]\ttraining's l1: 0.373261\tvalid_1's l1: 0.380665\n",
      "[3000]\ttraining's l1: 0.368897\tvalid_1's l1: 0.377826\n",
      "[3500]\ttraining's l1: 0.365794\tvalid_1's l1: 0.376095\n",
      "[4000]\ttraining's l1: 0.363349\tvalid_1's l1: 0.374965\n",
      "[4500]\ttraining's l1: 0.36131\tvalid_1's l1: 0.374105\n",
      "[5000]\ttraining's l1: 0.359562\tvalid_1's l1: 0.373488\n",
      "[5500]\ttraining's l1: 0.357881\tvalid_1's l1: 0.37294\n",
      "[6000]\ttraining's l1: 0.356517\tvalid_1's l1: 0.372588\n",
      "[6500]\ttraining's l1: 0.355197\tvalid_1's l1: 0.372259\n",
      "[7000]\ttraining's l1: 0.353893\tvalid_1's l1: 0.371976\n",
      "[7500]\ttraining's l1: 0.352674\tvalid_1's l1: 0.371712\n",
      "[8000]\ttraining's l1: 0.351513\tvalid_1's l1: 0.371502\n",
      "[8500]\ttraining's l1: 0.350353\tvalid_1's l1: 0.371318\n",
      "[9000]\ttraining's l1: 0.349194\tvalid_1's l1: 0.371111\n",
      "[9500]\ttraining's l1: 0.348165\tvalid_1's l1: 0.370991\n",
      "[10000]\ttraining's l1: 0.347012\tvalid_1's l1: 0.370827\n",
      "[10500]\ttraining's l1: 0.346002\tvalid_1's l1: 0.370737\n",
      "[11000]\ttraining's l1: 0.344987\tvalid_1's l1: 0.370641\n",
      "[11500]\ttraining's l1: 0.343976\tvalid_1's l1: 0.370592\n",
      "[12000]\ttraining's l1: 0.343004\tvalid_1's l1: 0.370512\n",
      "[12500]\ttraining's l1: 0.342052\tvalid_1's l1: 0.370451\n",
      "[13000]\ttraining's l1: 0.341094\tvalid_1's l1: 0.370398\n",
      "[13500]\ttraining's l1: 0.340155\tvalid_1's l1: 0.370347\n",
      "[14000]\ttraining's l1: 0.339175\tvalid_1's l1: 0.370282\n",
      "Early stopping, best iteration is:\n",
      "[14085]\ttraining's l1: 0.33904\tvalid_1's l1: 0.370266\n",
      "calculating CV\n",
      "Round: 6, CV score: 1123.91022\n",
      "start predicting\n",
      "##### round 7#####\n",
      "spliting data\n",
      "start training\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.467583\tvalid_1's l1: 0.464395\n",
      "[1000]\ttraining's l1: 0.415113\tvalid_1's l1: 0.414749\n",
      "[1500]\ttraining's l1: 0.391662\tvalid_1's l1: 0.394066\n",
      "[2000]\ttraining's l1: 0.379955\tvalid_1's l1: 0.38461\n",
      "[2500]\ttraining's l1: 0.373317\tvalid_1's l1: 0.379834\n",
      "[3000]\ttraining's l1: 0.368907\tvalid_1's l1: 0.377159\n",
      "[3500]\ttraining's l1: 0.365795\tvalid_1's l1: 0.375557\n",
      "[4000]\ttraining's l1: 0.363297\tvalid_1's l1: 0.374414\n",
      "[4500]\ttraining's l1: 0.36126\tvalid_1's l1: 0.373569\n",
      "[5000]\ttraining's l1: 0.359345\tvalid_1's l1: 0.372849\n",
      "[5500]\ttraining's l1: 0.357734\tvalid_1's l1: 0.372316\n",
      "[6000]\ttraining's l1: 0.356328\tvalid_1's l1: 0.371916\n",
      "[6500]\ttraining's l1: 0.355039\tvalid_1's l1: 0.371619\n",
      "[7000]\ttraining's l1: 0.35378\tvalid_1's l1: 0.37135\n",
      "[7500]\ttraining's l1: 0.352565\tvalid_1's l1: 0.371124\n",
      "[8000]\ttraining's l1: 0.351411\tvalid_1's l1: 0.370919\n",
      "[8500]\ttraining's l1: 0.350157\tvalid_1's l1: 0.370707\n",
      "[9000]\ttraining's l1: 0.349053\tvalid_1's l1: 0.370536\n",
      "[9500]\ttraining's l1: 0.347998\tvalid_1's l1: 0.370378\n",
      "[10000]\ttraining's l1: 0.346935\tvalid_1's l1: 0.370223\n",
      "[10500]\ttraining's l1: 0.345901\tvalid_1's l1: 0.370112\n",
      "[11000]\ttraining's l1: 0.344872\tvalid_1's l1: 0.370003\n",
      "[11500]\ttraining's l1: 0.343897\tvalid_1's l1: 0.369939\n",
      "[12000]\ttraining's l1: 0.342903\tvalid_1's l1: 0.369815\n",
      "[12500]\ttraining's l1: 0.341932\tvalid_1's l1: 0.369725\n",
      "[13000]\ttraining's l1: 0.34099\tvalid_1's l1: 0.369666\n",
      "[13500]\ttraining's l1: 0.340016\tvalid_1's l1: 0.369584\n",
      "[14000]\ttraining's l1: 0.339038\tvalid_1's l1: 0.369509\n",
      "[14500]\ttraining's l1: 0.338106\tvalid_1's l1: 0.369466\n",
      "[15000]\ttraining's l1: 0.337217\tvalid_1's l1: 0.369412\n",
      "[15500]\ttraining's l1: 0.336331\tvalid_1's l1: 0.369373\n",
      "[16000]\ttraining's l1: 0.335455\tvalid_1's l1: 0.369317\n",
      "Early stopping, best iteration is:\n",
      "[16084]\ttraining's l1: 0.335301\tvalid_1's l1: 0.36931\n",
      "calculating CV\n",
      "Round: 7, CV score: 1127.80658\n",
      "start predicting\n",
      "##### round 8#####\n",
      "spliting data\n",
      "start training\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.467024\tvalid_1's l1: 0.471848\n",
      "[1000]\ttraining's l1: 0.414738\tvalid_1's l1: 0.42066\n",
      "[1500]\ttraining's l1: 0.391466\tvalid_1's l1: 0.39867\n",
      "[2000]\ttraining's l1: 0.379849\tvalid_1's l1: 0.388452\n",
      "[2500]\ttraining's l1: 0.373157\tvalid_1's l1: 0.383128\n",
      "[3000]\ttraining's l1: 0.368724\tvalid_1's l1: 0.379954\n",
      "[3500]\ttraining's l1: 0.365705\tvalid_1's l1: 0.378027\n",
      "[4000]\ttraining's l1: 0.363154\tvalid_1's l1: 0.376639\n",
      "[4500]\ttraining's l1: 0.36102\tvalid_1's l1: 0.37562\n",
      "[5000]\ttraining's l1: 0.359293\tvalid_1's l1: 0.374855\n",
      "[5500]\ttraining's l1: 0.357783\tvalid_1's l1: 0.374255\n",
      "[6000]\ttraining's l1: 0.356332\tvalid_1's l1: 0.373724\n",
      "[6500]\ttraining's l1: 0.355\tvalid_1's l1: 0.37334\n",
      "[7000]\ttraining's l1: 0.353671\tvalid_1's l1: 0.372982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7500]\ttraining's l1: 0.352407\tvalid_1's l1: 0.372635\n",
      "[8000]\ttraining's l1: 0.351251\tvalid_1's l1: 0.372374\n",
      "[8500]\ttraining's l1: 0.350055\tvalid_1's l1: 0.372137\n",
      "[9000]\ttraining's l1: 0.348942\tvalid_1's l1: 0.371952\n",
      "[9500]\ttraining's l1: 0.347918\tvalid_1's l1: 0.371817\n",
      "[10000]\ttraining's l1: 0.346838\tvalid_1's l1: 0.371662\n",
      "[10500]\ttraining's l1: 0.345827\tvalid_1's l1: 0.371514\n",
      "[11000]\ttraining's l1: 0.344787\tvalid_1's l1: 0.371354\n",
      "[11500]\ttraining's l1: 0.343822\tvalid_1's l1: 0.371284\n",
      "[12000]\ttraining's l1: 0.342823\tvalid_1's l1: 0.371141\n",
      "[12500]\ttraining's l1: 0.341848\tvalid_1's l1: 0.371024\n",
      "[13000]\ttraining's l1: 0.340897\tvalid_1's l1: 0.370951\n",
      "[13500]\ttraining's l1: 0.339957\tvalid_1's l1: 0.370857\n",
      "[14000]\ttraining's l1: 0.339016\tvalid_1's l1: 0.37074\n",
      "[14500]\ttraining's l1: 0.338116\tvalid_1's l1: 0.370681\n",
      "[15000]\ttraining's l1: 0.337197\tvalid_1's l1: 0.3706\n",
      "[15500]\ttraining's l1: 0.336291\tvalid_1's l1: 0.370516\n",
      "[16000]\ttraining's l1: 0.335375\tvalid_1's l1: 0.370482\n",
      "[16500]\ttraining's l1: 0.334482\tvalid_1's l1: 0.370411\n",
      "[17000]\ttraining's l1: 0.333587\tvalid_1's l1: 0.37039\n",
      "[17500]\ttraining's l1: 0.332705\tvalid_1's l1: 0.370326\n",
      "[18000]\ttraining's l1: 0.33187\tvalid_1's l1: 0.370272\n",
      "Early stopping, best iteration is:\n",
      "[18090]\ttraining's l1: 0.331709\tvalid_1's l1: 0.370265\n",
      "calculating CV\n",
      "Round: 8, CV score: 1147.66036\n",
      "start predicting\n",
      "##### round 9#####\n",
      "spliting data\n",
      "start training\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.467663\tvalid_1's l1: 0.464835\n",
      "[1000]\ttraining's l1: 0.415282\tvalid_1's l1: 0.414273\n",
      "[1500]\ttraining's l1: 0.39189\tvalid_1's l1: 0.393195\n",
      "[2000]\ttraining's l1: 0.380187\tvalid_1's l1: 0.383541\n",
      "[2500]\ttraining's l1: 0.373496\tvalid_1's l1: 0.378728\n",
      "[3000]\ttraining's l1: 0.369129\tvalid_1's l1: 0.375981\n",
      "[3500]\ttraining's l1: 0.365906\tvalid_1's l1: 0.374224\n",
      "[4000]\ttraining's l1: 0.363403\tvalid_1's l1: 0.37302\n",
      "[4500]\ttraining's l1: 0.361341\tvalid_1's l1: 0.372189\n",
      "[5000]\ttraining's l1: 0.359672\tvalid_1's l1: 0.37156\n",
      "[5500]\ttraining's l1: 0.358115\tvalid_1's l1: 0.371039\n",
      "[6000]\ttraining's l1: 0.356644\tvalid_1's l1: 0.370619\n",
      "[6500]\ttraining's l1: 0.355199\tvalid_1's l1: 0.370241\n",
      "[7000]\ttraining's l1: 0.353856\tvalid_1's l1: 0.369937\n",
      "[7500]\ttraining's l1: 0.352638\tvalid_1's l1: 0.369657\n",
      "[8000]\ttraining's l1: 0.351382\tvalid_1's l1: 0.369396\n",
      "[8500]\ttraining's l1: 0.350277\tvalid_1's l1: 0.36921\n",
      "[9000]\ttraining's l1: 0.349127\tvalid_1's l1: 0.369026\n",
      "[9500]\ttraining's l1: 0.348047\tvalid_1's l1: 0.368885\n",
      "[10000]\ttraining's l1: 0.346983\tvalid_1's l1: 0.368757\n",
      "[10500]\ttraining's l1: 0.345912\tvalid_1's l1: 0.368619\n",
      "[11000]\ttraining's l1: 0.344898\tvalid_1's l1: 0.368493\n",
      "[11500]\ttraining's l1: 0.343905\tvalid_1's l1: 0.368418\n",
      "[12000]\ttraining's l1: 0.342896\tvalid_1's l1: 0.368311\n",
      "[12500]\ttraining's l1: 0.34193\tvalid_1's l1: 0.368251\n",
      "[13000]\ttraining's l1: 0.340972\tvalid_1's l1: 0.368218\n",
      "[13500]\ttraining's l1: 0.339997\tvalid_1's l1: 0.368108\n",
      "[14000]\ttraining's l1: 0.339066\tvalid_1's l1: 0.368074\n",
      "[14500]\ttraining's l1: 0.338168\tvalid_1's l1: 0.368034\n",
      "[15000]\ttraining's l1: 0.337277\tvalid_1's l1: 0.367977\n",
      "[15500]\ttraining's l1: 0.336354\tvalid_1's l1: 0.367901\n",
      "Early stopping, best iteration is:\n",
      "[15575]\ttraining's l1: 0.336217\tvalid_1's l1: 0.367896\n",
      "calculating CV\n",
      "Round: 9, CV score: 1118.88363\n",
      "start predicting\n",
      "cv scores of 10 rounds:\n",
      "[1130.6873928466473, 1141.5700404319077, 1144.110102961624, 1132.2963517185367, 1143.132798503863, 1136.9136465284687, 1123.9102218152577, 1127.8065776874614, 1147.6603642666412, 1118.8836314714342]\n",
      "preparing output\n",
      "time:  1550170885\n"
     ]
    }
   ],
   "source": [
    "prediction = 0\n",
    "lst = []\n",
    "\n",
    "rpt = 10\n",
    "for i in range(rpt):\n",
    "    print('##### round {:d}#####'.format(i))\n",
    "    print('spliting data')\n",
    "    time0 = round(time.time())\n",
    "    x_train, x_valid, y_train, y_valid =train_test_split(X, y, test_size=0.1, random_state=time0)\n",
    "    d_train = lgb.Dataset(x_train, label=y_train, categorical_feature = categorical_columns)\n",
    "    d_valid = lgb.Dataset(x_valid, label=y_valid, categorical_feature = categorical_columns)\n",
    "\n",
    "    print('start training')\n",
    "    num_round = 10000\n",
    "    param['num_thread'] = 3\n",
    "    param['feature_fraction'] = 0.3149\n",
    "    model = lgb.train(param, d_train, num_round, valid_sets = [d_train, d_valid], verbose_eval=500)\n",
    "    gc.collect()\n",
    "\n",
    "    print('calculating CV')\n",
    "    oof  = np.exp(model.predict(x_valid, num_iteration=model.best_iteration)) - shift\n",
    "    cv = mean_absolute_error(np.exp(y_valid)-shift, oof)\n",
    "    print(\"Round: {:d}, CV score: {:<8.5f}\".format(i, cv))\n",
    "    \n",
    "    lst.append(cv)\n",
    "    \n",
    "    print('start predicting')\n",
    "    prediction += np.exp(model.predict(X_test)) - shift\n",
    "    \n",
    "print('cv scores of {:d} rounds:'.format(rpt))\n",
    "print(lst)\n",
    "print('preparing output')\n",
    "submission = pd.DataFrame()\n",
    "submission['loss'] = prediction/rpt\n",
    "submission['id'] = ids\n",
    "\n",
    "time0 = round(time.time())\n",
    "tm = str(time0) #+ '_' + str(round(cv*10))\n",
    "print('time: ',tm)\n",
    "#submission.to_csv('submit_'+ tm +'.csv', index=False)\n",
    "submission.to_csv('submit_final_2.csv.gz', compression='gzip', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1130.6873928466473,\n",
       " 1141.5700404319077,\n",
       " 1144.110102961624,\n",
       " 1132.2963517185367,\n",
       " 1143.132798503863,\n",
       " 1136.9136465284687,\n",
       " 1123.9102218152577,\n",
       " 1127.8065776874614,\n",
       " 1147.6603642666412,\n",
       " 1118.8836314714342]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
