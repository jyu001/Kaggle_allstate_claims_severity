{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4, Models Stacking and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macssd/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#tf.python.control_flow_ops = tf\n",
    "\n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "_stdout = sys.stdout\n",
    "\n",
    "sys.path.append('modules')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from scipy.stats import iqr\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers import Dense, Dropout, Activation, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "from xgboost import XGBRegressor\n",
    "#from xgb_regressor import XGBoostRegressor\n",
    "#from stacker import Stacker\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To follow conventional function names in sklearn, we implement fit and predict functions\n",
    "\n",
    "def xg_eval_mae(yhat, dtrain):\n",
    "    y = dtrain.get_label()\n",
    "    return 'mae', mean_absolute_error(np.exp(y), np.exp(yhat))\n",
    "\n",
    "class XGBoostRegressor(object):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.params = kwargs\n",
    "        if 'num_boost_round' in self.params:\n",
    "            self.num_boost_round = self.params['num_boost_round']\n",
    "        self.params.update({'silent': 1, 'objective': 'reg:linear', 'seed': 0})\n",
    "        \n",
    "    def fit(self, x_train, y_train):\n",
    "        dtrain = xgb.DMatrix(x_train, y_train)\n",
    "        self.bst = xgb.train(params=self.params, dtrain=dtrain, num_boost_round=self.num_boost_round,\n",
    "                             feval=xg_eval_mae, maximize=False)\n",
    "        \n",
    "    def predict(self, x_pred):\n",
    "        dpred = xgb.DMatrix(x_pred)\n",
    "        return self.bst.predict(dpred)\n",
    "    \n",
    "    def kfold(self, x_train, y_train, nfold=5):\n",
    "        dtrain = xgb.DMatrix(x_train, y_train)\n",
    "        cv_rounds = xgb.cv(params=self.params, dtrain=dtrain, num_boost_round=self.num_boost_round,\n",
    "                           nfold=nfold, feval=xg_eval_mae, maximize=False, early_stopping_rounds=10)\n",
    "        return cv_rounds.iloc[-1,:]\n",
    "    \n",
    "    def plot_feature_importances(self):\n",
    "        feat_imp = pd.Series(self.bst.get_fscore()).sort_values(ascending=False)\n",
    "        feat_imp.plot(title='Feature Importances')\n",
    "        plt.ylabel('Feature Importance Score')\n",
    "        \n",
    "    def get_params(self, deep=True):\n",
    "        return self.params\n",
    " \n",
    "    def set_params(self, **params):\n",
    "        self.params.update(params)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macssd/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/macssd/anaconda3/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.cross_validation import KFold, train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers import Dense, Dropout, Activation, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "from xgboost import XGBRegressor\n",
    "#from xgb_regressor import XGBoostRegressor\n",
    "\n",
    "class Stacker(object):\n",
    "    def __init__(self, xgboost_func, mlp_func, train_path='train.csv', seed=0, test_size=0.25, **kwargs):\n",
    "        self.seed = seed\n",
    "        self.test_size = test_size\n",
    "        self.xgboost_func = xgboost_func\n",
    "        self.train_path = train_path\n",
    "        self.mlp_func = mlp_func\n",
    "        self.mlp_fit_kwargs = kwargs.get('mlp_fit_kwargs', {'nb_epoch': 30, 'batch_size': 128, 'verbose': 1})\n",
    "        self.mlp_predict_kwargs = kwargs.get('mlp_predict_kwargs', {'batch_size': 256, 'verbose': 1})\n",
    "\n",
    "    def stack_and_compare(self):\n",
    "        xg_xtr, xg_xte, xg_ytr, xg_yte  = self.preprocess(encoding='label', transform_label=True)\n",
    "        mlp_xtr, mlp_xte, mlp_ytr, mlp_yte = self.preprocess(encoding='one-hot', transform_label=False)\n",
    "        \n",
    "        assert mean_absolute_error(np.exp(xg_ytr), mlp_ytr) < 0.001 # Sanity check\n",
    "        assert mean_absolute_error(np.exp(xg_yte), mlp_yte) < 0.001 # Sanity check\n",
    "\n",
    "        xgb_folds = self.predict_folds(self.xgboost_func, xg_xtr, xg_ytr)\n",
    "        mlp_folds = np.log(self.predict_folds(self.mlp_func, mlp_xtr, mlp_ytr, \n",
    "                           fit_kwargs=self.mlp_fit_kwargs, predict_kwargs=self.mlp_predict_kwargs))\n",
    "\n",
    "        xgb_pred_hold = self.predict_holdout(self.xgboost_func, xg_xtr, xg_ytr, xg_xte)\n",
    "        mlp_pred_hold = np.log(self.predict_holdout(self.mlp_func, mlp_xtr, mlp_ytr, mlp_xte, \n",
    "                               fit_kwargs=self.mlp_fit_kwargs, predict_kwargs=self.mlp_predict_kwargs))\n",
    "\n",
    "        score_xgb, score_mlp = self.evaluate_estimators(xgb_pred_hold, mlp_pred_hold, xg_yte)\n",
    "        print ('Single model performance:', 'xgb:', score_xgb, ',', 'mlp:', score_mlp)\n",
    "\n",
    "        stacker = self.stack(xgb_folds, mlp_folds, xg_ytr)\n",
    "        score_stacker = self.evaluate_stacker(stacker, xgb_pred_hold, mlp_pred_hold, xg_yte)\n",
    "\n",
    "        print ({'xgb': score_xgb, 'mlp': score_mlp, 'stacker': score_stacker})\n",
    "        return {'xgb': score_xgb, 'mlp': score_mlp, 'stacker': score_stacker}\n",
    "\n",
    "    def preprocess(self, encoding='one-hot', transform_label=False):\n",
    "        train = pd.read_csv(self.train_path)\n",
    "        if transform_label: \n",
    "            train['loss'] = np.log(train['loss'])\n",
    "        cat_features = [x for x in train.select_dtypes(include=['object']).columns if x not in ['id','loss']]\n",
    "        if encoding == 'one-hot':\n",
    "            train = pd.get_dummies(data=train, columns=cat_features)\n",
    "        elif encoding == 'label':\n",
    "            for c in range(len(cat_features)): train[cat_features[c]] = train[cat_features[c]].astype('category').cat.codes\n",
    "        else:\n",
    "            raise Exception(\"Correct value of 'encoding' is required. Possible values of encoding=['one-hot', 'label']\")\n",
    "        features = [x for x in train.columns if x not in ['id','loss']]\n",
    "        train_x = np.array(train[features])\n",
    "        train_y = np.array(train['loss'])\n",
    "        x_tr, x_te, y_tr, y_te = train_test_split(train_x, train_y, test_size=self.test_size, random_state=self.seed)\n",
    "        return x_tr, x_te, y_tr, y_te\n",
    "\n",
    "    def predict_folds(self, model_func, xtrain, ytrain, fit_kwargs={}, predict_kwargs={}):\n",
    "        folds = KFold(len(ytrain), shuffle=False, n_folds=3)\n",
    "        fold_preds = np.zeros(len(ytrain))\n",
    "        for k, (train_index, test_index) in enumerate(folds):\n",
    "            xtr = xtrain[train_index]\n",
    "            ytr = ytrain[train_index]\n",
    "            estimator = model_func(xtrain.shape[1])\n",
    "            xte, yte = xtrain[test_index], ytrain[test_index]\n",
    "            estimator.fit(xtr, ytr, **fit_kwargs)\n",
    "            fold_preds[test_index] = estimator.predict(xte, **predict_kwargs)\n",
    "        return fold_preds\n",
    "\n",
    "    def predict_holdout(self, model_func, xtrain, ytrain, xtest, fit_kwargs={}, predict_kwargs={}):\n",
    "        estimator = model_func(xtrain.shape[1])\n",
    "        estimator.fit(xtrain, ytrain, **fit_kwargs)\n",
    "        return estimator.predict(xtest, **predict_kwargs)\n",
    "\n",
    "    def evaluate_estimators(self, xgb_pred, mlp_pred, test_y):\n",
    "        return mean_absolute_error(np.exp(xgb_pred), np.exp(test_y)), mean_absolute_error(np.exp(mlp_pred), np.exp(test_y))\n",
    "\n",
    "    def stack(self, xgb_oof, mlp_oof, oof_y):\n",
    "        assert len(xgb_oof) == len(mlp_oof)\n",
    "        oof_x = np.vstack((xgb_oof, mlp_oof)).T\n",
    "        metaestimator = LinearRegression()\n",
    "        metaestimator.fit(oof_x, oof_y)\n",
    "        return metaestimator\n",
    "\n",
    "    def evaluate_stacker(self, stacker, xgb_pred, mlp_pred, holdout_y):\n",
    "        holdout_pred = np.hstack((xgb_pred.reshape(len(xgb_pred), 1), np.array(mlp_pred)))\n",
    "        predictions = stacker.predict(holdout_pred)\n",
    "        score = mean_absolute_error(np.exp(predictions), np.exp(holdout_y))\n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the final part of this capstone, we implement a simple stacker model in which we generate predictions using two models: XGBoost and MLP. Then we combine them in a single dataset and train a second level algorithm to get the final prediction.\n",
    "\n",
    "We are going to do a K-Fold stacking in which we train each model (level 0 model or just L0-model) on a subset of data, generate out-of-fold predictions and train the stacker (level 1 model, L1-model) on these predictions.\n",
    "\n",
    "As usual, use can use pretrained models and skip the heavy computation phase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_PRETRAINED = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "Our methodology which I took from [MLWave Ensembling Guide](http://mlwave.com/kaggle-ensembling-guide/) as well as from the winners of [Otto Group Classification Challenge](https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/14335/1st-place-winner-solution-gilberto-titericz-stanislav-semenov) is as follows:\n",
    "\n",
    "* **Splitting.** Split the training set into K folds\n",
    "\n",
    "\n",
    "* **Out-of-fold predictions**. Fit each L0-model on K-1 folds, generate predictions for the other fold. Repeat the process for all K folds. In the end, we get predictions for the whole training set (for which we also have labels).\n",
    "\n",
    "\n",
    "* **Fitting on the entire training set**. We fit each L0-model on the whole training set and get predictions for the test set. We combine predictions into a dataset, in which each feature is prediction of a single L0-model.\n",
    "\n",
    "\n",
    "* **Training L1**. We fit L1-model on out-of-fold predictions, while using corresponding labels from the training set as labels for L1. After that, we ask the L1-model get the final prediction using our combine dataset of L0 predictions.\n",
    "\n",
    "There is only one complication: we don't have the test set (we're not going to submit our prediction to Kaggle now), but we need to compare the performance of single L0-models with the stacker, as we'd like to make sure that L1-model works better than any of L0-models.\n",
    "\n",
    "To do this, we split our training dataset into train and test subsets. We will touch the test subset only once when we compare the performance of all the models (L0 vs L1).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting\n",
    "\n",
    "### Preparing the data\n",
    "We trained our models on the same train set, but the preprocessing phase has been done differently for XGBoost and MLP. We have to replicate the same preprocessing for our ensemble phase. Let's revisit what we have to do for each model:\n",
    "\n",
    "* **XGBoost**: 1) log-transform the target feature, 2) use a label encoding for categorical features.\n",
    "\n",
    "\n",
    "* **MLP**: 1) use an one-hot encoding for categorical features.\n",
    "\n",
    "I'll be verbose in this section and apply all the transformations manually without writing an abstraction (say, a function or a class). We only have two models and there's no need to abstract the logic in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's see that we haven't messed up with data split:\n",
      "Training set X: (141238, 130) Y: (141238,)\n",
      "Test set X: (47080, 130) Y: (47080,)\n"
     ]
    }
   ],
   "source": [
    "# XGBoost\n",
    "\n",
    "trainxg = pd.read_csv('./data/train.csv')\n",
    "ntrain = trainxg.shape[0]\n",
    "\n",
    "trainxg['log_loss'] = np.log(trainxg['loss'])    \n",
    "features = [x for x in trainxg.columns if x not in ['id','loss', 'log_loss']]\n",
    "\n",
    "cat_features = [x for x in trainxg.select_dtypes(\n",
    "        include=['object']).columns if x not in ['id','loss', 'log_loss']]\n",
    "\n",
    "for c in range(len(cat_features)):\n",
    "    trainxg[cat_features[c]] = trainxg[cat_features[c]].astype('category').cat.codes\n",
    "\n",
    "trainxg_x = np.array(trainxg[features])\n",
    "trainxg_y = np.array(trainxg['log_loss'])\n",
    "\n",
    "# xg_xte, xg_yte are for the final performance check\n",
    "xg_xtr, xg_xte, xg_ytr, xg_yte = train_test_split(trainxg_x, trainxg_y, test_size=0.25, random_state=31337)\n",
    "\n",
    "print (\"Let's see that we haven't messed up with data split:\")\n",
    "print (\"Training set X:\", xg_xtr.shape, \"Y:\", xg_ytr.shape)\n",
    "print (\"Test set X:\", xg_xte.shape, \"Y:\", xg_yte.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's see that we haven't messed up with data split:\n",
      "Training set X: (141238, 1153) Y: (141238,)\n",
      "Test set X: (47080, 1153) Y: (47080,)\n"
     ]
    }
   ],
   "source": [
    "# MLP\n",
    "\n",
    "trainmlp = pd.read_csv('./data/train.csv')\n",
    "cat_names = [c for c in trainmlp.columns if 'cat' in c]\n",
    "\n",
    "trainmlp = pd.get_dummies(data=trainmlp, columns=cat_names)\n",
    "\n",
    "features_mlp = [x for x in trainmlp.columns if x not in ['id','loss']]\n",
    "\n",
    "trainmlp_x = np.array(trainmlp[features_mlp])\n",
    "trainmlp_y = np.array(trainmlp['loss'])\n",
    "\n",
    "mlp_xtr, mlp_xte, mlp_ytr, mlp_yte = train_test_split(trainmlp_x, trainmlp_y, test_size=0.25, random_state=31337)\n",
    "\n",
    "print (\"Let's see that we haven't messed up with data split:\")\n",
    "print (\"Training set X:\", mlp_xtr.shape, \"Y:\", mlp_ytr.shape)\n",
    "print (\"Test set X:\", mlp_xte.shape, \"Y:\", mlp_yte.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the L0-models\n",
    "\n",
    "Now we just take our final models for XGBoost and MLP. Of course, we could have found more models to add to the stacker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "\n",
    "reg_xgb = XGBoostRegressor(num_boost_round=200, eta=0.07, gamma=0.2, max_depth=8, min_child_weight=6,\n",
    "                colsample_bytree=0.6, subsample=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP\n",
    "\n",
    "def hyper_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(351, input_dim=len(features_mlp), init='glorot_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.578947))\n",
    "    \n",
    "    model.add(Dense(293, init='glorot_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.26666))\n",
    "    \n",
    "    model.add(Dense(46, init='glorot_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.188888))\n",
    "    \n",
    "    model.add(Dense(1, init='glorot_normal'))\n",
    "    model.compile(loss='mae', optimizer='adadelta')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out-of-fold predictions\n",
    "\n",
    "Now we divide our training set `xg_xtr, xg_ytr` (xgboost), `mlp_xtr, mlp_ytr` (mlp) into k-folds (3), train on 2/3 folds and predict the third fold. We persist the results to files to get back to them later on.\n",
    "\n",
    "We also save test labels (`*_test_fold_*`) to make sure that the fold generator splits data the same fashion for XGBoost and MLP and we can stack their predictions side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "if not USE_PRETRAINED:\n",
    "    folds = KFold(len(xg_ytr), shuffle=False, n_folds=3)\n",
    "\n",
    "    for k, (train_index, test_index) in enumerate(folds):\n",
    "        xtr = xg_xtr[train_index]\n",
    "        ytr = xg_ytr[train_index]\n",
    "        xte, yte = xg_xtr[test_index], xg_ytr[test_index]\n",
    "        reg_xgb = XGBoostRegressor(num_boost_round=200, eta=0.09, gamma=0.1, max_depth=8, min_child_weight=6,\n",
    "                        colsample_bytree=0.9, subsample=0.8)\n",
    "        reg_xgb.fit(xtr, ytr)\n",
    "        np.savetxt('ensemble/xgb_pred_fold_{}.txt'.format(k), np.exp(reg_xgb.predict(xte)))\n",
    "        np.savetxt('ensemble/xgb_test_fold_{}.txt'.format(k), yte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP\n",
    "if not USE_PRETRAINED:\n",
    "    folds = KFold(len(mlp_ytr), shuffle=False, n_folds=3)\n",
    "    for k, (train_index, test_index) in enumerate(folds):\n",
    "        xtr = mlp_xtr[train_index]\n",
    "        ytr = mlp_ytr[train_index]\n",
    "        xte, yte = mlp_xtr[test_index], mlp_ytr[test_index]\n",
    "        reg_mlp = hyper_model()\n",
    "        fit = reg_mlp.fit(xtr, ytr, batch_size=128, nb_epoch=30, verbose=0)\n",
    "        pred = reg_mlp.predict(xte, batch_size=256)\n",
    "        np.savetxt('ensemble/mlp_pred_fold_{}.txt'.format(k), pred)\n",
    "        np.savetxt('ensemble/mlp_test_fold_{}.txt'.format(k), yte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on the whole dataset\n",
    "\n",
    "We train the same models on the whole training set (`xg_xtr, mlp_xtr` and corresponding labels) and generate predictions for the test set (`xg_xte, mlp_xte`). Remember that we do have labels for the test set, but we don't allow our L0-model see them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "if not USE_PRETRAINED:\n",
    "    reg_xgb = XGBoostRegressor(num_boost_round=200, eta=0.09, gamma=0.1, max_depth=8, min_child_weight=6,\n",
    "                  colsample_bytree=0.9, subsample=0.8)\n",
    "    reg_xgb.fit(xg_xtr, xg_ytr)\n",
    "    np.savetxt('ensemble/xgb_pred_test.txt'.format(k), np.exp(reg_xgb.predict(xg_xte)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP\n",
    "if not USE_PRETRAINED:\n",
    "    reg_mlp = hyper_model()\n",
    "    fit = reg_mlp.fit(mlp_xtr, mlp_ytr, batch_size=128, nb_epoch=30, verbose=0)\n",
    "    pred = reg_mlp.predict(mlp_xte, batch_size=256)\n",
    "    np.savetxt('ensemble/mlp_pred_test.txt'.format(k), pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1-model training\n",
    "\n",
    "When the previous stage is completed, we have generated out-of-fold and test set predictions, which we can now use to train the stacker.\n",
    "\n",
    "First, we load predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_xgb1 = np.loadtxt('ensemble/xgb_pred_fold_0.txt')\n",
    "train_xgb2 = np.loadtxt('ensemble/xgb_pred_fold_1.txt')\n",
    "train_xgb3 = np.loadtxt('ensemble/xgb_pred_fold_2.txt')\n",
    "\n",
    "train_mlp1 = np.loadtxt('ensemble/mlp_pred_fold_0.txt')\n",
    "train_mlp2 = np.loadtxt('ensemble/mlp_pred_fold_1.txt')\n",
    "train_mlp3 = np.loadtxt('ensemble/mlp_pred_fold_2.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check \\#1\n",
    "We load labels to check that we haven't messed up with folds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_test_fold1 = np.exp(np.loadtxt('ensemble/xgb_test_fold_0.txt'))\n",
    "xgb_test_fold2 = np.exp(np.loadtxt('ensemble/xgb_test_fold_1.txt'))\n",
    "xgb_test_fold3 = np.exp(np.loadtxt('ensemble/xgb_test_fold_2.txt'))\n",
    "\n",
    "mlp_test_fold1 = np.loadtxt('ensemble/mlp_test_fold_0.txt')\n",
    "mlp_test_fold2 = np.loadtxt('ensemble/mlp_test_fold_1.txt')\n",
    "mlp_test_fold3 = np.loadtxt('ensemble/mlp_test_fold_2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreating the original set of training set labels:\n",
    "\n",
    "xgb_test_fold = np.hstack((xgb_test_fold1, xgb_test_fold2, xgb_test_fold3))\n",
    "mlp_test_fold = np.hstack((mlp_test_fold1, mlp_test_fold2, mlp_test_fold3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1180932118489165e-12"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And testing that these labels completely match (there's still a little rounding error due to log-exp conversion):\n",
    "\n",
    "mean_absolute_error(xgb_test_fold, mlp_test_fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is basically zero. Labels from two fold generators match and we can go on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check \\#2\n",
    "\n",
    "Now we load the predictions for the whole test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_xgb = np.loadtxt('ensemble/xgb_pred_test.txt')\n",
    "test_mlp = np.loadtxt('ensemble/mlp_pred_test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We should check that MAE from combined out-of-fold predictions is reasonable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined out-of-fold predictions for XGBoost and MLP\n",
    "\n",
    "train_xgb_folds = np.hstack((train_xgb1, train_xgb2, train_xgb3))\n",
    "train_mlp_folds = np.hstack((train_mlp1, train_mlp2, train_mlp3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1155.1388944371015"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MAE of XGBoost combined predictions\n",
    "\n",
    "mean_absolute_error(np.exp(xg_ytr), train_xgb_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1147.367184299818"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MAE of MLP combined predictions\n",
    "\n",
    "mean_absolute_error(mlp_ytr, train_mlp_folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Predictions are around 1150-1155: this is exactly what we expect from these models.\n",
    "\n",
    "We do the same for the test set and get scores for each single model. The score of the best single model is our baseline score for the stacker (our stacker should perform better than any given single model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline MAE which we need to improve with stacking, XGB: 1152.2006603472637; MLP: 1148.0359189095834.\n"
     ]
    }
   ],
   "source": [
    "mae_xgb_test = mean_absolute_error(np.exp(xg_yte), test_xgb)\n",
    "mae_mlp_test = mean_absolute_error(mlp_yte, test_mlp)\n",
    "print (\"Baseline MAE which we need to improve with stacking, XGB: {}; MLP: {}.\".format(mae_xgb_test, mae_mlp_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions on test should be close to 1145-1150."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training L1-model\n",
    "\n",
    "Finally, it's time to join the predictions of L0-models and train a stacker over them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_train_x = np.vstack((train_xgb_folds, train_mlp_folds)).T\n",
    "l1_test_x = np.vstack((test_xgb, test_mlp)).T\n",
    "l1_train_y = mlp_ytr\n",
    "l1_test_y = mlp_yte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain shape: (141238, 2)\n",
      "ytrain shape: (141238,)\n",
      "Xtest shape: (47080, 2)\n",
      "ytest shape: (47080,)\n"
     ]
    }
   ],
   "source": [
    "# Just a sanity check\n",
    "print (\"Xtrain shape:\", l1_train_x.shape)\n",
    "print (\"ytrain shape:\", l1_train_y.shape)\n",
    "print (\"Xtest shape:\", l1_test_x.shape)\n",
    "print (\"ytest shape:\", l1_test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of the algorithm for L1 is crucial. The problem is that we don't want it overfit on the training set, and it can be very easily done. To reduce the possibility of overfitting, we should take a very simple regressor, and LinearRegression is the ideal candidate for us.\n",
    "\n",
    "We now fit a very basic linear regression and get the predictions for the final test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1020.42718506  999.43988037]\n",
      " [1167.49963379 1045.68322754]\n",
      " [1069.88916016 1060.41333008]\n",
      " ...\n",
      " [3103.23144531 2856.31298828]\n",
      " [2170.16210938 1978.93310547]\n",
      " [1309.57043457 1438.5657959 ]] [ 986.06 1753.14 1792.7  ... 3394.84 1587.2  1602.7 ] [[2618.94726562 2671.17944336]\n",
      " [2938.3972168  3014.97021484]\n",
      " [1104.91955566 1443.08862305]\n",
      " ...\n",
      " [3623.34277344 3300.77807617]\n",
      " [1988.98425293 2522.15234375]\n",
      " [ 581.17321777  847.07281494]] [  47.20170534   71.47767004 -134.8282622  ...  115.67379634  -10.83454155\n",
      " -270.34439429]\n"
     ]
    }
   ],
   "source": [
    "reg = LinearRegression()\n",
    "\n",
    "# Note that normalizing the data in case of linear models is very important\n",
    "reg.fit(np.log(l1_train_x), np.log(l1_train_y))\n",
    "pred = reg.predict(np.log(l1_test_x))\n",
    "\n",
    "\n",
    "print(l1_train_x, l1_train_y, l1_test_x, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3XmcE+X9wPHPN8keciinB4egotajVRSvar3rVettRW21VcvPqq3VWqUe1Vq1orVQvC/Eq6IiCgqKnAqCwHKDXAsssFy7sLucu+yR5/dHJrtJJpNMsskmy37fr9dC8sxk5slkZr4zzzVijEEppZQK5cl0BpRSSmUfDQ5KKaVsNDgopZSy0eCglFLKRoODUkopGw0OSimlbDQ4KKWUstHgoJRSykaDg1JKKRtfpjOQrE6dOpmePXtmOhtKKdVszJ49e4sxprObeZttcOjZsycFBQWZzoZSSjUbIrLG7bxarKSUUspGg4NSSikbDQ5KKaVsNDgopZSy0eCglFLKRoODUkopGw0OSimlbDQ4KKVUlvlq0Sa27NyT0TxocFBKqSyyo6qG29+bzc1DZmY0HxoclFIqi9T5DQDF5ZUZzYcGB6WUUjYaHJRSStlocFBKKWWjwUEppZSNBgellFI2GhyUUkrZaHBQSqksYkymcxCgwUEppbKQSGbXr8FBKaWUjQYHpZRSNhoclFJK2WhwUEopZaPBQSmllI0GB6WUUjYaHJRSKotkSTcH98FBRLwiMldEvrDeHyIiM0RkhYh8KCK5Vnqe9b7Qmt4zZBl/s9KXiciFIekXWWmFItI/dV9PKaWaF2P1gstwN4eE7hzuBpaEvB8ADDTGHA6UA7da6bcC5caYXsBAaz5E5GigL3AMcBHwkhVwvMCLwMXA0cD11rxKKdViSYZ7wbkKDiLSDfgF8Ib1XoBzgeHWLG8DV1ivL7feY00/z5r/cmCYMWaPMWY1UAicbP0VGmNWGWOqgWHWvEoppTLE7Z3DIOB+wG+97whUGGNqrffFQFfrdVdgHYA1fZs1f316xGec0pVSSmVI3OAgIpcCJcaY2aHJUWY1caYlmh4tL/1EpEBECkpLS2PkWimlVGO4uXM4HbhMRIoIFPmcS+BOop2I+Kx5ugEbrNfFQHcAa/p+QFloesRnnNJtjDGvGWP6GGP6dO7c2UXW1d5sVelOphVuyXQ2lNorxQ0Oxpi/GWO6GWN6EqhQnmiMuRGYBFxjzXYzMNJ6Pcp6jzV9oglUv48C+lqtmQ4BDgdmArOAw63WT7nWOkal5Nupvdq5z33DDW/MyHQ2lNor+eLP4ugBYJiIPAHMBd600t8E3hWRQgJ3DH0BjDGLReQj4AegFrjTGFMHICJ3AWMBLzDEGLO4EflSSinVSAkFB2PMZGCy9XoVgZZGkfNUAdc6fP5J4Mko6WOAMYnkRSml9kbNrhOcUkqpptOcOsEppZRqITQ4KKWUstHgoJRSykaDg1JKKRsNDkoppWw0OCilVBYxWdKWVYODUkploQyP2K3BQSmlsonJkm5wGhyUUiorNYOH/SillGpZNDgopZSy0eCglFLKRoODUkopGw0OSimlbDQ4KKWUstHgoJRSykaDg1JKZZPs6AOnwUEppbKRDp+hlFIq62hwUEopZaPBQSmllI0GB6WUUjYaHJRSKotkSWMlDQ5KKZWNMtxYSYODUkplE31MqFJKKUfaz0EppVTW0eCglFLKRoODUkopGw0OSimlbDQ4KKWUstHgoJRSykaDw15m2My1zFlbnulsKKWaOV+mM6BSq/+IhQAUPf2LDOdEKZUMkyUDaOidg1JKZSHJ8AAacYODiOSLyEwRmS8ii0XkH1b6ISIyQ0RWiMiHIpJrpedZ7wut6T1DlvU3K32ZiFwYkn6RlVYoIv1T/zWVUkolws2dwx7gXGPMccDxwEUiciowABhojDkcKAdutea/FSg3xvQCBlrzISJHA32BY4CLgJdExCsiXuBF4GLgaOB6a969xpade/jjB3PZtac201lRSilX4gYHE7DTeptj/RngXGC4lf42cIX1+nLrPdb080RErPRhxpg9xpjVQCFwsvVXaIxZZYypBoZZ8+41Bo5bzufzNzBi7vq0rmfMwo1pXf7ers6fHWW9SmUDV3UO1hX+PKAEGAesBCqMMcFL4WKgq/W6K7AOwJq+DegYmh7xGaf0aPnoJyIFIlJQWlrqJustyh3vz8l0FpqtkfPWc9iDY1i9ZVems6JauGY1Kqsxps4YczzQjcCV/lHRZrP+j1aLYpJIj5aP14wxfYwxfTp37hw/40q5FLzrWrZpe4ZzolRAsxqV1RhTAUwGTgXaiUiwKWw3YIP1uhjoDmBN3w8oC02P+IxTulJKqQxx01qps4i0s17vA5wPLAEmAddYs90MjLRej7LeY02faIwxVnpfqzXTIcDhwExgFnC41fopl0Cl9ahUfDmllGpusqRUyVUnuIOAt61WRR7gI2PMFyLyAzBMRJ4A5gJvWvO/CbwrIoUE7hj6AhhjFovIR8APQC1wpzGmDkBE7gLGAl5giDFmccq+oVJKNUOZfkxo3OBgjFkA9I6SvopA/UNkehVwrcOyngSejJI+BhjjIr9KqRDlu6rxiLBfq5xMZ0XtZXT4DKWasd7/HAfocCkq9XT4jKaURBu18l3VHPePr5m3riINGdr7jJhTzFUvfZfpbKgWbE9tHevKdsedb+qKLVndt0aDQxNoTJO071dtZVtlDa9MXpm6DO3F7v1oPnPWOgfS2WvKWVm603G6im3S0hLOfW4yNXX+TGcla/3lo/n87JlJVNXUOc4zYclmfv3mDF6fsqoJc5YYDQ6qRbn65Wmc99w3mc5Gs/XQpwtZVbqLkh17Mp2VrPXNskAH3eoYAXTT9ioA1myNf4eRKRoc9iKZ7jTTElz98jQuGKjBRe39tEJaqQTMXqMPUlItg945NIFsGStFKZX9TJacMDQ4NKVmWO7z8uSVjJyX3tFklVJ2kuHzhRYrqZgGfLUUgMuPjzpQrmqhsuXqtrlqDptP7xyaUnPYI5SKIdNXs8matnILPfuPZm0TtA5K5CjP5s2pwaEJZPMOoFRL8MnsQNHojNVbm2ydyR722XINqcFBKRXVtsoabhk6i5IdVbZp2XICU+mjwUGpvdii9dvo2X80C4u3JfzZjwvWMXFpCa9Mzt5evCp9NDgotRcbv2QzAOOs/7OVMYbvV23Viu4sosEhy+mh0jT0nJRZH88upu9r3zNqvj4EMltocNiLaL13KuhWjCVdjSvWbN0F4Go002QE850t1wDZko9YNDhkOT1VKdV49cdRlp2VYx3fmW7lqMGhCWXZfqmUUo40ODQBacT1fzYGFGMMc9bqAHRK7c00ODQBk4JTfKZvMUN9VLCOq16axleLNmY6K6oJbK+qYdO28L4OWoGfGtm8GTU4NKHGnN+z6WAsLAk8SW1tmioPVXYZPruYU/81AQit2M2iHTIB2ZLvLLrWc6TBIcs1h52oJdq2u4bHP/+B6tqW9bjMxhSRupGui6D6oJbG2FCawNPxsiNExabBIcs1h52oJRowdilDvlvNZzqceUqkO+ike/kTl27mpCfHM3lZSUKfy+aLPw0OzUQ21TlkUxFXptRazwduNj16m0s+m6l5aysAmL9uW6P3iWz5qTQ4NBPZssOESvfVmGq85vIbZUtdQCo1dnjzTF8QanBoQsns/onsH811rH3V/KS7bkBlngaHJtBcrt7c2vuu8ZRbzf3k3Rz23Wy5i9LgkOWyYzeJrrmfKFRsmSjKbM6tlVIt0xeVGhyaCT0RK5W8pjp+XF/1N4MopcEhDeasLefPw+bi96duB2gG+1KjzV5Txr0fzWs+LYBasFT/Qpm+Sm60KNHHzX6czRd9GhzS4Jahs/hs3ga2VdY0elnZuO+k69x905szGTFnPbuq69Kzgiy1c09tSi8k0ikb90eVHi0yOFTV1NGz/2je+35NprMSV/M4ZShXSpeBP7xHddmuao59dCyDJ67IUKaS01zv7pqysre5tx5skcGhbFc1AC9OKmyS9e1tA++pJGxcAC+eDFP/E5a8ZWdgyIXRC6IPYriubDeDxi9P+8l42sotzFi1Ne58zfeE13zynS1xt0UGh0zJloH3du6pbdTns6WpXSp5TC1neuanbwXbigP/FxewoaLS9Tg8t71dwKDxK1izNb2DHN7w+gyue+37tK4jG2TLideNTMdhDQ5NKN2d4NyYvnIrxz46lm+Wlyb82Tq/4e5hc/lmWeCzXy9O7UPrM3ncXl72Fu/kDqDDlllpX9dPn57ISU+OdzVvVW3Lqn9Jl8acaF+aXMii9dtczes2+DjNtnTTdkYvzI6h8OMGBxHpLiKTRGSJiCwWkbut9A4iMk5EVlj/t7fSRUQGi0ihiCwQkRNClnWzNf8KEbk5JP1EEVlofWawNNG9a1NdRTTVw37crKWgqAyAWavLwtJLdlTxy+en2sbtD7WhopKR8zawakvgeb8zi8oc521u9q8JDKCXuyfd3ykzITCZte6Nd4iR/H7DpKUlMYvtnvlqGZc+PzXmcpI9wiPPDRcNmsKzY5fZ5iss2cmlz09he1XjG7m45ebOoRb4izHmKOBU4E4RORroD0wwxhwOTLDeA1wMHG799QNehkAwAR4FTgFOBh4NBhRrnn4hn7uo8V/NWaZv15KR7jx/MGMdC9dv438zwivpc6jFS9NcvTbDn8W1Gqs10qyiMjz4EVI/1Pfdw+bybkQji3TtN80tbKyznj0ybeWWsPQh363md0Nn8dWiTZnIlusAPHD8chat315/194U4gYHY8xGY8wc6/UOYAnQFbgceNua7W3gCuv15cA7JuB7oJ2IHARcCIwzxpQZY8qBccBF1rR9jTHTTSB8vxOyrKxRUFTm+tGYFbubLrqn24r8m/gi98FMZyOqAb7XWJB3a6az4cqiDdsB2FZZw/K8mxif+9eUr2PkvA088tmilC83VDDWZHPZ/Qcz1/LjR8eGNQ+ead0pj1kYHgRqi+dQlH8DuzelrrWYm03THC6EEqpzEJGeQG9gBnCAMWYjBAIIsL81W1dgXcjHiq20WOnFUdLTLlrUNsbwzfJS223mNa9M56qXpsVd5tpGVBzOXlPOI58tinqLm8mD8SjPuvgzNZG3pxXR97XpAFznm8y+UpnhHCVGAJ/4Oczjrlw5607C1llNanbBY/vB/GGZzU8Ufx+5iB17aqkNCQ5Od1C9t44BoOuW2MVGyUgkAKwr283PnpnovCzj52gpanSeEuE6OIhIG+AT4M/GmO2xZo2SZpJIj5aHfiJSICIFpaXJ317FqgMYPruYm4fM5OOCYsd5Ytm0vaHMPtHj+ppXpvHu92syM6ZNFhQUuMnBo6MW8/2q5ljXIda/yW3nVBcP1db56x/3mgzfjg2BF1OeS1GO9g7JHkX/m7mWdWXOFzrnbB3GmLwHab91bpJrSJyr4CAiOQQCw/vGmBFW8marSAjr/+AjkIqB7iEf7wZsiJPeLUq6jTHmNWNMH2NMn86dO7vJesKKywM/0PqK7LoidXNyuMnzFYV5vybWLhp353VY0ZiFG7l/+IL4mcgi//l6WX1xQnOVrqD9zNhlnP+fb1izdZdt2sh563l58sqUrm/b7hrKrf5F2WTJph0pWY6kubwtr3QhAPtUNl1LJjetlQR4E1hijAntwTMKCLY4uhkYGZJ+k9Vq6VRgm1XsNBa4QETaWxXRFwBjrWk7RORUa103hSyrxQu2h3czpMTffW/jEz9ubmgjY0C8ffqO9+cw3UUnqUTMKirjhw0NN6GpLocdPLGQX706PaHPFKwpZ+C45SnOSeOleuyhWVZLsy077Sfsu4fNY8BXS1O6vuMe/5re/xznON1XV8WDvvfx1Dm3lnMyeMIKevYfHbNTa7Y9hrQ51Eu4uXM4HfgNcK6IzLP+LgGeBn4uIiuAn1vvAcYAq4BC4HXgDgBjTBnwT2CW9fe4lQbwB+AN6zMrgS9T8N3iSiTIe6nD08gWJm7W903un5FPGipZB44PnKi+TaJfQjKacoe89pXpXDJ4Stz5jDF8VLCOHSHN+Mb/kNo+FkEFReX8d4JD5eS3z9Lf90FE3hJbfrLFStkntd/j5I3v0s83muM2fJjwZ1/7dhWArQloQnddGarcSTxoNV0+ffFmMMZMxfmccV6U+Q1wp8OyhgBDoqQXAMfGy0uqxCqicdr08/L6UUUO8MuUrg8CbZjP/883APTwlMDiT+Ba22ZKq2w+Zc1dV8H9wxcwdUVDM8Tb3imgKL+JMzLxCW73wdO11yd8UC/dtJPeJB58M1khHW3d6bp48PgDgd9jGtd7H4InXHcbzqTxciiVP10mdgPtIe0g8oTeVirpLNvZvL2Knv1HM3tN6sqy562riJr+xYINMZvFrq+obPRQGJGcAtlhsp6LPTNSuq5I0Q6Arxdvqh93KDgOUTbw+hMrP/96SfQ7Hbcn/4v/+y3XN2J4i+ra1PSryDHV9JLkGms0HZNwP5JsvDjK9DhWLTo4GAL9F0q2uy/nDHaieXe6+xFdk736u+t/sVsmnP70RK55OX7z2kTUOQwdPSHvr7yc+19Xyzji4S+5639zUpKffu/O5s2pq1OyrFQ5RZbw1A/ncYosSfizTsVK8c4Du6rrGlXn8+q3q5IaMiXSfVXPMz7vfrx7ol/QNFYqit0GeF9mdf6vU5Ab99pWbWJy7j202dNwERDrJ83fs5UpuXfTqSp7R4ZukcEh9Ee75pXpXPzfhnLvwpJA64V0DK+fjguBpVFaW3TE3TgwYawI9vzEwrDiGzfWV1Ty/IQV9f0zqmv9fOEwymgs8TZPUw4dEMtPvYsBONXzQwKfSu9VoJsLnGmFif2u7Xatoij/BnpLQx3MT+oC311qkuvPE611FLgr3nH7fJQrPd8GlpngMby9qoZP5yZ3V3TM5k/p6dnMMSVfuJr/4JIJdPeUclppoI7FdV6b8BanRQaHSFtDmtgFe1A67cTRFJfvZvUW9/On26S8vzhOc9oJQ5O/WV4SfSYHt787m+fGLWdlaXq3waL1sbrXNJ27fSPiz5Ri+7KTNjifkE9+akLU9PinXOezTfetgbvSX3hDi7MaF+TOenZyUh1FFxRXcNw/vmbkvPVJDV8e98LMGO7/eAH3fDifJRvTv58l+hXSWTfipOUFB2OQyvLgS0eJ/BRnDJjEOf+eHGVVTRfmQ9e1r8Q/+FK5q1XW1NnysLdYUFzB5S9+1+gy+2S3d3CTLsjvx6L82xqVh1TkJ8jpl/5o1rq49XGlOxNvrhq8MLh72DwO+dsYKnYnVufTmTKK8m/gOHFu7rppexVHyDo862fGXNY704v4PqKIL7g9F6zfxm4Xzc4l4v9s1PKCw8zX6Pzyj+ghyQ20Fe/8FzxB/so7iX0/uBQAr7+Ga7zfJHS58FGB85AVxhhOjhjy+VWrOV882dAT2km0nE3MvZcvc/vb0n2ktiLeyWUvfMf8dRWMmGMvbhBxvy2NNK6HtJMLPbM42zPP3cypPBM57Mv3f7KAq19OrG8JxL6yn7BkMw9+ujAsLbT+xM3X+imBz9/kGxfzIubrvAc48our69/fNMQeKP4+cjF9HRoHVCbYQMRE+eIe/IzL/Stn1Nq3Y1M2hW5xwaFi3ucA9JDGtZOP15LgmZzXybGuQM4seZd/57xKz41fuV5+rN7IdX5DScTDYpyeJOYoRv7TeQNQsqOKG17/PmZv2dCcHerZxFGetbZ5ZuXdEXM9yd7FPJzzHp2xV7Y2vg4qPdeIr+YOZGjuM0Cg6CmP5Hohd2ELBxF5NdyEd74xts+9H0V/CFNx+W5Kdri7C2m4mDC8PsXdhRS471+UcLFPjP2zLbs53LOev1a9kPzyU6DFBYd15el9olY0bWoDt9m5NSkoy/z0drwv9mn8ciKE7qtvTF0d87kOjstwMc+bU1czbeVWhs2KcWfkYjntJflxgWLpJlt4KudNW7qvtun3m0Rc4vmeBfn9GJn7SNhQFW4fFjQt/09Mz/9jWJqp/9/18GeO7vR+xlVWRXGqAuUZAyZx8pPR61oi3SGfAHCOZ17YQ6pu9DbcgSfaYGTod41pRRfcftlbsNTigkO48B28qib+gdS4q2qHZoyJLGL+B0hZase9AXtx0ziHdvkQ6C3+L9/rdJPAVVUi+a+tc9iAleW0JjvGs/JRy8Li8BZfvdZ93LiFWhvpDE96htR+KXcwAD/yrAsbqmJ7ZUMxR2jHvdTdEzj/+lU1dXxcsA5jDH/N+Yj/5L6SsrVG6u/7gKL8G8LSQo/VgyRwV9Qh4qIiVxqO+bj78ZIvoOi7+rePfR67tVposCnfVR21RVkm7gjcittDuqVYs3UXz33tfkydRH7S0B1gW2UNhSU7OLFHhwSW4CI/MTI0celmBOGcH+2fkiKjUzxLuN43iYOlBPhtfbrTspdt2kHPTq3I83nr+yxs2hYRCAb0ZJYHyIeddZtIx675wkR3Y/YL8MsXplL09C/izudWcB/wJFBP0Vju+u8keXJysSM9/eVShk4rYv998znL5WIlyR30dt/n0Sfs2AStOrlYgonf6ezDG60X/3OcxakoLhisg/tUtFUdLUUIUGxi5VfrHNIu8rc569nJjJofPhjs2MWb6Nl/ND37j65PM8bPrd7R5PoTL2YQDLcOncXVL09nT5Tb/VxS346/ts7PLUML+N3QwLORg+MGpfJ6JXRH/82b4b2oS3ZUceGgb3n40/Ar5rdjdSKsTbxIy41/JxD8U83pjmmzdQIPPScWluyo74zYmBZgTs1bA+sIXEGv2Ow8KmnwRPcr72Qe8r0XyE8Ce06wPmBnlYtKWhNY7syiMopTVfRbWQ7PHQlfPxSW7Lr4aObrFOXfQBt2sx+xizET7s1s/a4NRXeGMXkPMjrvwagBRofPaGKxjrutu6r5v3dn29K7bJrIIznv88uSxG+Ru5ZOYf8NE6Kuu5cU89+cF6J8ytklnu85TNbHnGfc4k309U4kn9hDT6SqEnpKRAe64ImhYI3zU/S+WhRemS4Enu37vNMAeM1AyY4qVpY2nFCcNm9ka5jlm3dw/n++ZXCKvntocWHo+WuH9bu4eZj9frKb3/vGhKUFK+ir66I38Y0ZdGKcR2vrDLe9XRA3T2F1Yg7Lq95pNald1jCOp9+I+3GxZgSO8dl5tzM/v5+7z7jmXOfwM89Caw4dPiMj4p0Lyx3aUXutIYU91Q07/2GynmOcntK0viHAdNv6HS95/22bpWPZHMbn3c/F3lmO+TlaimD4LWFpL+UOZkLeX6FwPNsjeo/+YvAUVpXu5KvP3uXpnDdso4nGk8xumWwz2fe+D2+NZIDJyzZTMjGxYNkYP6pqGKokFa10Tn5yAuc99039e6cDPYda7vJ+So4J7G/Buo7gI2nPr5tiK0tPxIzIByMt+RxePdNx7KF3phe5Wm7w5LzRoeHCzwd+GzU96LpXp/PJ7EDz4O1VNfwQ0vGssqaOnv1Hc/ewubw4qRB/lCsXp/HIQv3mZfuT1QyBuxN7uvOpOE+SazZdnuTjgn/qWew47Z1pRdz4RvJjbCWixQYHN3rLCoryb6Cvt2En22iVlW/eXsUya+iKCXl/ZXSew3OWXz+XXdWxO1C12eVcxNKB7SzNu5kxeQ/Cok+iz/Te1RRF9DpdvGE7v3+nAH9V4KDrKLFbSkUefgYcmwlGnjzdXIklWjyy7/op/DNnaEKfCV9fYvO39scuNtiTooHrIv3aO477cj7muuoRGGP4y8fhzTb71SYW1COtiHza2ye3wcb55FpNOyPv9P4+0vnEFC64PcI3dA61roa2n7G6rP67PjB8ATur7SfgkfM28OzYZdZdjuFa7+SEil7f9z4WNY/pFNr35fSnJzJlhUNTWGsHjTb8TS+PvTTgnJC+LN8Vpva5Kk5aXHAItkiKdzpbtH4753sDV/1P57xRn14bcht94aBvHQeqCyXlyTd5O8WzhHxJ7gpkQ0X0k3sX4o+x88aUVXGbCbo55K5/3X6VE+zkc6FnJnlUMzXKmD/eOnvLpUwOUhn9d3beAh78rjrq7WP1S8iLKPZLR1+TN6asCnuucpDzaLf2eVtZQdQpfyvyb+L9nKcAw88qRtGG3bY7ysifMbLPTjSXeGbwbM5rvJf7lG2al+itDNtGeb640/2BGBNl/2r8Rc+8tdHvcIJLjpbHkzzLbavfz8WoB6nW4oLD7pCr+MYcf8HfLfKkEW2Z53jtnXjKQjuBpenCpjJK09yzPfOYlv8nOq4N75AXuY+vcTn+zfqKSpZZ5cvRjpPN2+0Hfhurk8+ruYNYlv/btDw4PVpRBAQepjTIqtvZsXYRu0rsgTvhYqWSpbCloY5g7dbdjMp9mML8m2J/bukY4v34qdo1essKaur8YRc3QZFFko4rXz62/mTWcJK1n0BP8/7A2Z75XF86iMdzhoZNK8q/Ae+exAeGDA4Jc7JnGW0jxph6mDeifaSBi41oSO5pccEHDa22jperPIFiwFYELsz+M345/xrjPHrv1V7nh11FC2TaQ7qJhJ6g7/CO5A5v/KeTRv5ga8siTqL+OldlxP3ebah0i3VF7LQzdCW54ZeDJ+Idq2fx4ay1UU8WsXSXzexv9SAW4MnRiYxMGt2xnvATdGQdRFAih+4Nb8xgRd5v+LNveFh6D08JV3gDA8q1HXI6rV86PqG8huodHKfnpVPghUDHxG2VNVz6/BSO9RSFzRv1inXY9ZzjDRQXhPZHCMwf+N1TcTK40DOLT/Me5Trv5Ljd2XKpoQMORZCb7X00evjXRb0qCPbaPlZW1xe/Bj0+xKF4FLg3ZziHlMcehn5uXj/a+LfRht3c7h3FNYTc4c55J+ZnY1Xy5ifRAvHTuYEioGBx88GewHF5oATqNYxxGNrGuD/uNkY2+24iLS44OB1s9+d8yP05DY8oPEaK6stlY7n8halh79uWunuOQfgVtfMJ4LaIViJB3+Xf7Wo9kYfC/Tkf1b9+4JOFXPHSd0TKpYZPch/l+CiDlE3Ju4eBuS9HXVesO+zQSfFO8s9PXMH6KM0ZvdTytO+1+s53QXPXlnP60xPDHiM6c3UZOVICNRA/AAAX3klEQVTHnxs5gupBbHW8sznTGz7ez6rSnRz3j6/ZHtJ0c13Zbmb9sJJ/+IZGXUYHAifOaMU9qXKwNVTMYbIhzpzwSs5A5uTfzqO+tzltlfPzO0zoyW2d80B1R3jW8/zE8P3oXt9wXskZ6PiZx3xvx8yjT/z8aNdsFuXfRv+cYeETR/3RNv+OPfGLZScsKeHajc+EJ7otx1zwMUdJ9AsaN8Yu3hTWazvSvR9GHz4k3VpsJ7jQcspu0jBE9QDfa1znm+z4uakrSrk2t+H9ruo6CHtcpbuD3O1V8Ake51EkQ53hWRhzeuQVU/Bd5DDY/byfs5/s4kTPCv6Zk55HlR4SZ9DDy73T6FZub1Fykiylr28y3SOCw3NfL2d9RaWrFixB5Stm0N7FfMEhJb6Te+LOu2pzYFsGixQALhj4LQ+b17jRF7vJsZOo+8nqb6FtF+jUy8USDBd4A3epoRXFx0gRI/Ie49I9T7Cu7CQOs9LPte5kfucb6z6TCfZL+anXutsMO2fHPiIacwe1rbKGttbiYy2lm7h73sUDvg94tvY6TpDlDK94HEbAhd5Ec9WQk/om8xGPva31w+7qWmr96WkMEU+Lu3MIGpLb0KT0ZyEn1liBIVS0ncyfwNVfvDnv8n7KT8T9MBkH4NSPwF2evHWV9Pf9jwdzPuBO3yhXn3Gz5JNkKZd5AsUENVYRVrwD/amcNzl+86eu8pCs9u9f4DitPn9LGh7cklcbvagl9JscuC4w/w/5DU2OK2vqXLXesS23fsFRttXbv4QXTnS1nKs9U+orOM/1NDTXPc8buMMdnPMCs2P0QXGrzm8YOS96APw09+8xP9vav5PrvJNs6fns4XfeL6M2u01FhX3ow4MeyXmP3i4vxP7g+5yzPPO53mdvKhvpVu/osIclJaKTbKfVUx05s7Kh2EzrHNIoWpnjnb74dQ2RrvBO4/u8O8N23Nveid95pz4fxulNwH05HzMq75GE8+W4vjhXZj9d/xa3+9w9xcqtMwZM5OO8xxmc+wLGwL+/XgbASZ6lYfO5vYtymm9ZjA5XAKxN7NnX+4t1B1I/XAJ4/NFbxPxtRMOFhdQlPiJqvN8l2QZafb0T+Zfvdbp7Gu6K86WaYLBpZbWOOtSzKakT7dQVDc0p35m+hsMeHMPdw6IPHR7rpLtm6y7u3D6QA6UhQAWz84BvGI/mvMuFngLbdrqt5AnXeXU6of7FYbRXwCoqc976bgP+Iznv82neo4DVSnDSvxKObH/c1nAh20r2xO3QmiotLjiE/qh9JHCScns7CeEH84FSzhO+t+rfT1ya2BPUABat38aHMZ7dkC4/kZUNxWnFBXSuTLy57SGeTexXu4WFebdypKy1NVksLg+vSFtuVUw+mNO4tvuRSq2mkI7H3BDnuwQAUxdeJn2kx/7sho67419VLlxvb4UT6+Ey0LA/CaY+/wfLZmhEUcJnuQ/zdM4bXO+b5Hh6Cy026rI98TLtHzY2fNcvFyf3bBQIDFvjrQw//g7xbOZ27yj2k8CTBV/JHcSTPvtIucmoP37raimpiHFR8ebPo7b2c+twCd+HjpEipuX/Cb55GlPyQ9gYUkfIOo50WWfxRM5bzIwzXH2qtLjg8DNvQ4uLwbkvNHo8oxt9Dbd83WUzZTvctXgQCTyw5N6PXD6oJUEe/LyZ8ywneZZFnX6GdzFT8/4cePPGeRxREbtHazRdZSs/3jmNtlLJTd5xcSqkG387/FbOACD2rXUrqrjIM5MHEugRXllp/80m5IY/arXH1qm2eSIFezWHGpn3dzrF6YAIgaIKCPS2/zbvHq7YOSzOJ5wd72loHXNaSG9bp0DRtjrxi5p0F2/cGtEQw5uiAQvzpJYpuXfDK6czqvyKmPOW7ox3J+h8Z/Fq7qCw96GdZFds3kloceHXeQ8wNs/+QCsn+0bpG5EOLbZCGqCLlLE8/+aULW9K3j0UTD0ibshdkHcbx+14jZutcXV6J1yZZXeKJ7wtdWcqOM8712Hu1HF7ktjHVFK6Myfp9TzmGxo2vHKof+e8wn01twPwdM7rXOaN8iSyx/ajmwyypwPexcNtaYd5Enx4EnAg5USrI/i51z5GV1Dk3F2tu9gDK+YwZUVpfUVx0K49tbROIE8nO1wchGtchefRsobDvBt4r+7nCX+2C1vo40nvgIiR+2h3TymUJtcUPOjN3OeS/qxTH5xs0+LuHNLNzY6+r+ymtxRyqIumhW79yvdNzOnp2h3zN9nrWc7zzLY9kezhPYNsLaOCBuS8Hnc9v/V97TjtGm/grqfL0reiB4b6fEUPlnlf3ht3/W5c7f2Wovwb488YIladw2/enEnkL/fIyOSfB3GQlCU9TlCk0Hw/kvMeT+S8FWPu6Nqym7+GNB+3ryM1ukjsZ1o3tYEffsVJK5yb8mYLDQ4ZMiLvMSbm3Ze25XeR8PFXrvBOCxt6PFVyIvqCLMi7jTdzn+Mh3/th6V1r1/FLz7RGDSIXT685T6Zt2W708CRePBN6ki3dUcW/rKFazvQu5N2cp2xXvaGjkW5LcmC3SO0q3dV5bQvpqNdZ3DcbdvJB7hNc6bX3swHIpwZvI+9oGitdo6K+mpv9gQE0OGSFdJTfBltIhLrCE7/cPFGXegMtgYJ1L8FhDm7yjbPNe6U39evfm3y5cCNdQ4J6aP1YUGiJRP8Rzs8ZT8Q+te6Gs5g3raES+1SP85AQbkX2Ig/VVirre7I3V2/kPJvpLDSKBocEPZeTvkcdptug3JdsaaEdACP9OMbBG41nt3Orr1RehdV3okrQ/tL49vypdnDI9n/si/jfa/qqhuBRkaI7B7fOoqHu5KoWEOh71MV4IJUL5zdBnV86aXBIUI5DpWhjNGXHlkjt4zzhKhG+Kuey3V6eDfgcRs9sKm479zWlVhK7zfpBMUbQzSmK3wnLjVQF7mx5BrhKDQ0OGfYbr3NFa1O435d8k8lIa8fEbsEROSZSKl3qca6ITqdjJPnh2CN1chrwLsQJ0tDg4Z3cASlZb6ouTRbn35qiJalsoMEhwxrzQJtUiFaunazzK78Me3+H97Ow94d6ku8sFc8Luc+nbdmxjM57KP5MLhXk/yHuPCPyHkvZ+pSKRYNDFvizz3kI4+YsdARYlcWaR7N71cQ0OGSBA1LQLFCpZGlsUNFocFCqhatK0/OxVfOmwUEppZSNBgellFI2cYODiAwRkRIRWRSS1kFExonICuv/9la6iMhgESkUkQUickLIZ2625l8hIjeHpJ8oIgutzwwWcftsPqWUUuni5s5hKHBRRFp/YIIx5nBggvUe4GLgcOuvH/AyBIIJ8ChwCnAy8GgwoFjz9Av5XOS6lFJpdNZWbVWm7OIGB2PMt0Bk19fLgeBTwN8GrghJf8cEfA+0E5GDgAuBccaYMmNMOTAOuMiatq8xZroxxgDvhCxLKaVUhiRb53CAMWYjgPX//lZ6VyB0iMdiKy1WenGUdKWUUhmU6grpaPUFJon06AsX6SciBSJSUNrIh3UopZRylmxw2GwVCWH9HxxashjoHjJfN2BDnPRuUdKjMsa8ZozpY4zp07lz5ySzrpRSKp5kg8MoINji6GZgZEj6TVarpVOBbVax01jgAhFpb1VEXwCMtabtEJFTrVZKN4UsSymlVIbEfYa0iHwAnA10EpFiAq2OngY+EpFbgbXAtdbsY4BLgEJgN/A7AGNMmYj8E5hlzfe4MSZYyf0HAi2i9gG+tP6UUkplUNzgYIy53mHSeVHmNcCdDssZAgyJkl4AHBsvH0oppZqO9pBWSillo8FBKaWUjQYHpZRSNhoclFJK2WhwUEopZaPBQSmllI0GB6WUUjYaHJRSStlocFBKKWWjwUEppZSNBgellFI2GhyUUkrZaHBQSillo8FBKaWUjQYHpZRSNhoclFJK2WhwUEopZaPBQSmllI0GB6WUUjYaHJRSStlocFBKKWWjwUEppZSNBgellFI2GhyUUkrZaHBQSillo8FBKaWUjQYHpZRSNhoclFJK2WhwUEopZaPBQSmllI0GB6WUUjYaHJRSStlocFBKKWWjwUEppZSNBgellFI2WRMcROQiEVkmIoUi0j/T+VFKqZYsK4KDiHiBF4GLgaOB60Xk6HSs69u6H6djsWm3xr+/q/l2mby0rH+W/4i0LFc1P+/VnscndT9jqb+7bVrw+Npjcvii7hTm+Q9lfF1vtpt9bMfe27U/B6DAfwQP1Py+Pn1JxHLn+Hux2N+DKpNDhWldn/7f2qsocNgvN5t2bDbtAHis5ib+VH0nd1ffwcS64ynwH8FM/5H18670HxT22UqTC0ChvwsAG0wHBtZcTbHpVP+dv6n7Sf383/uPYkTdGWw0HerThtZeAMA204rhdWcCsNvkUWw6UWw62fL7ad3pYe8rTGvKTRvG1/W2zfuL2mejfudU8zXJWuI7GSg0xqwCEJFhwOXAD6le0ewzh/BhyU4G9T2ePbV+Hv98MfvkePnNaT1YvGE7AHcPm8c+OV7OPrIzRx20L384+zCqa/14PcL4JZvpfXB7amr9vD9jDVNWbOGVX5/I7DXl9OjYilq/4cNZ66jzG352eCeu7N2Vxz5fzHvfr2Va/3Pp3DaP575eznUndeecf08GYNB1x9O9Qyv+NXI2yzdsZTtteOrKH9OhdQ63vzeHGQ+ex5K1FTw6ay25Xg/X9unOjFVbeWPqagAGXnccJxzcnpo6w8zVZZzeqyNnPTuZ608+mEd/eTR+Y2iV6+PZsUt5cdLK+m1x5zmHsWVHNR8WrKNHx1bcd8GRTFiymYcvPZprXp7Gvvvk0Lt7O3od0JYzD+/EzjZ5tM71Urarmo5t8lhXtpvCkp3cMXQKXWQrn//jFlrlRt+l1ldUcuHAb/n1qT04+ZD23DK0AIBju+7LU1f+mKMP2heAkfM2MKuojN+feSjryyvZUVXLsk3b6dA6l7OO3B9jDJOXlVJcXkmuz8ONpxxMZU0dPTq2YteeOl75ZiXLN+/gT+cdzlUvTQNg+RMXM3dtOb32b4PfwKIN21izZRftWuVyRe+u9XncuK2S0/41EYCHf3EUM1aXcXLPDhzbdT8O6dSadq1yqPMbWuV6Gb+khJ4dW3H4AW0BqKyuY59cL9MKt/DS5JWcd9T+nNGrEy9/s5IBV/+EOr9h/JLNnHlEZ1aV7uLTOcVs3VXNFws28s8rjuWRzxbx0f+dxq9enY7XI9T5DZPuO5tbh87iptN6cOOpPdhRVYvXI+T5PNT6DeW7qtlTW0fF7hqWbd5Bz46tOb1XJ24dOovuHVrxlwuOoLrWz1nPTubD/zuVez+cz2XHd2HJxu2ccHB7ruzdlcLSnXRptw+tc71MWlbCPR/O591bT6Z1no/e3dsxc3UZvfZvw+7qOhYUb2P0wg0M7tub7VW1rCrdyY/fmsWRB7bl8t5dOeHgdmzdvIOb5m7g8cuO4fBaPwe1y6dLdR07/YYz2+1Tv62fHP0DZx7RmUWtcvlRp9Zcu3E7jy34Hdf26cb2ylo4rCNbdu4hx+Nhz8btdGq3D+v9fs577hsA/vf7U7jrkI4IULC2nHemr+GqE7rSoXUuRxzQlgNyvPXretQYRIQFxRVsqKikz7GBYPBd4RaeHL2EK3t35ZKvl/H+bacwv3gbV/XuyrVDZnDL6YfQ64RudAHuAYbPLg4cQzecwFk9O7Bl5x5a5/o4NdfLe9+v4b5FG7nomAP51UndOX3rbn70wlRe/vWJHN02nyNe/I4Rd/yU7u1b4fFAhT+wH4+av4HLjuvCsV32pa5zG6548Ts6tcllyG9PYtT8DZzYoz1luT4qdlfTs2Nrtu6qZnTb9FwARhJjTJOsKGYmRK4BLjLG3Ga9/w1wijHmLqfP9OnTxxQUFDRVFrNOnd/w1ner+fWpPcgPORAyobBkBxOXltDvzMNcf6aqpg6fR/B503fzWlvnxwA5CaxjXdlu2uT5aN86N235imZ3da1jYG1K5buqm/y7q6YjIrONMX3czJv5vTFAoqTZopaI9AP6ARx88MHpzlNW83qE2352aKazAUCv/dvSa/+2CX2mKQJaMoGne4dWachJfNkQGAANDKpeVtQ5AMVAaEFjN2BD5EzGmNeMMX2MMX06d+7cZJlTSqmWJluCwyzgcBE5RERygb7AqAznSSmlWqysuJc1xtSKyF3AWMALDDHGLM5wtpRSqsXKiuAAYIwZA4zJdD6UUkplT7GSUkqpLKLBQSmllI0GB6WUUjYaHJRSStlkRQ/pZIhIKbAmyY93ArakMDvppHlND81remhe0yNVee1hjHHVSazZBofGEJECt13IM03zmh6a1/TQvKZHJvKqxUpKKaVsNDgopZSyaanB4bVMZyABmtf00Lymh+Y1PZo8ry2yzkEppVRsLfXOQSmlVAwtKjhk6jnVItJdRCaJyBIRWSwid1vpHURknIissP5vb6WLiAy28rlARE4IWdbN1vwrROTmkPQTRWSh9ZnBIhLtGRmJ5NkrInNF5Avr/SEiMsNa74fW6LmISJ71vtCa3jNkGX+z0peJyIUh6Sn7HUSknYgMF5Gl1vY9LVu3q4jcY/3+i0TkAxHJz6btKiJDRKRERBaFpKV9WzqtI4m8PmvtBwtE5FMRaZfsNkvmd0kkryHT7hMRIyKdsmG7hjHGtIg/AqO9rgQOBXKB+cDRTbTug4ATrNdtgeUEnpX9DNDfSu8PDLBeXwJ8SeAhSKcCM6z0DsAq6//21uv21rSZwGnWZ74ELm5knu8F/gd8Yb3/COhrvX4F+IP1+g7gFet1X+BD6/XR1jbOAw6xtr031b8D8DZwm/U6F2iXjdsV6AqsBvYJ2Z6/zabtCpwJnAAsCklL+7Z0WkcSeb0A8FmvB4TkNeFtlujvkmherfTuBEaiXgN0yobtGpa/xpxAmtOftfHGhrz/G/C3DOVlJPBzYBlwkJV2ELDMev0qcH3I/Mus6dcDr4akv2qlHQQsDUkPmy+J/HUDJgDnAl9YO92WkAOvfltaO/dp1mufNZ9Ebt/gfKn8HYB9CZxwJSI967YrgeCwzjq4fdZ2vTDbtivQk/ATbtq3pdM6Es1rxLQrgfejbYt42yyZ/T2ZvALDgeOAIhqCQ8a3a/CvJRUrBQ/OoGIrrUlZt6G9gRnAAcaYjQDW//tbsznlNVZ6cZT0ZA0C7gf81vuOQIUxpjbK8uvzZE3fZs2f6HdIxqFAKfCWBIrA3hCR1mThdjXGrAf+DawFNhLYTrPJzu0aqim2pdM6GuMWAlfRyeQ1mf09ISJyGbDeGDM/YlLWbNeWFBxcPac6rRkQaQN8AvzZGLM91qxR0kwS6QkTkUuBEmPMbBf5iTUt7XklcOV2AvCyMaY3sIvA7bOTTG7X9sDlBIo1ugCtgYtjLD+T29WNrM2fiDwE1ALvB5MSzFMy+3si+WsFPAT8PdrkBPOUtu3akoKDq+dUp4uI5BAIDO8bY0ZYyZtF5CBr+kFASZy8xkrvFiU9GacDl4lIETCMQNHSIKCdiAQfDhW6/Po8WdP3A8qS+A7JKAaKjTEzrPfDCQSLbNyu5wOrjTGlxpgaYATwU7Jzu4Zqim3ptI6EWRW1lwI3Gqs8JYm8biHx3yURhxG4SJhvHWfdgDkicmASeU3fdk2m/LQ5/hG4ylxl/SjByqdjmmjdArwDDIpIf5bwCqNnrNe/ILxSaqaV3oFAGXt762810MGaNsuaN1gpdUkK8n02DRXSHxNeQXeH9fpOwivoPrJeH0N4JeAqAhWAKf0dgCnAkdbrx6xtmnXbFTgFWAy0spb1NvDHbNuu2Osc0r4tndaRRF4vAn4AOkfMl/A2S/R3STSvEdOKaKhzyPh2rc9XY08gzemPQEuA5QRaKDzUhOs9g8Ct3gJgnvV3CYGyygnACuv/4I8twItWPhcCfUKWdQtQaP39LiS9D7DI+swLuKgkc5Hvs2kIDocSaBVRaB04eVZ6vvW+0Jp+aMjnH7Lys4yQVj6p/B2A44ECa9t+Zh04WbldgX8AS63lvUvgZJU12xX4gEB9SA2BK9Jbm2JbOq0jibwWEiiXDx5jryS7zZL5XRLJa8T0IhqCQ0a3a+if9pBWSill05LqHJRSSrmkwUEppZSNBgellFI2GhyUUkrZaHBQSillo8FBKaWUjQYHpZRSNhoclFJK2fw/Q9TN1MM8T2sAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(l1_train_x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(l1_train_y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1020.42718506  999.43988037]\n",
      " [1167.49963379 1045.68322754]\n",
      " [1069.88916016 1060.41333008]\n",
      " ...\n",
      " [3103.23144531 2856.31298828]\n",
      " [2170.16210938 1978.93310547]\n",
      " [1309.57043457 1438.5657959 ]] [ 986.06 1753.14 1792.7  ... 3394.84 1587.2  1602.7 ] [[2618.94726562 2671.17944336]\n",
      " [2938.3972168  3014.97021484]\n",
      " [1104.91955566 1443.08862305]\n",
      " ...\n",
      " [3623.34277344 3300.77807617]\n",
      " [1988.98425293 2522.15234375]\n",
      " [ 581.17321777  847.07281494]] [-5.96727313e+28  4.24492733e+29 -1.78960940e+30 ...  8.27448533e+29\n",
      " -2.70075348e+29 -2.62895483e+30]\n"
     ]
    }
   ],
   "source": [
    "reg = LinearRegression()\n",
    "\n",
    "# Note that normalizing the data in case of linear models is very important\n",
    "reg.fit(l1_train_x, l1_train_y)\n",
    "pred = reg.predict(l1_test_x)\n",
    "\n",
    "print(l1_train_x, l1_train_y, l1_test_x, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.92797663 6.907195  ]\n",
      " [7.06261968 6.95242576]\n",
      " [6.97531033 6.96641405]\n",
      " ...\n",
      " [8.04019925 7.95728691]\n",
      " [7.68255715 7.59031314]\n",
      " [7.17745445 7.27140192]] [6.8937172  7.46916374 7.49147814 ... 8.13001191 7.36972674 7.37944499] [[7.87052771 7.89027539]\n",
      " [7.98561955 8.01134523]\n",
      " [7.00752781 7.27454097]\n",
      " ...\n",
      " [8.1951523  8.1019135 ]\n",
      " [7.59537936 7.83286792]\n",
      " [6.36504885 6.74178666]] [-4.24358289e+11 -6.90736526e+11  1.14078211e+12 ... -9.83537493e+11\n",
      " -1.30911949e+11  2.41602495e+12]\n"
     ]
    }
   ],
   "source": [
    "print(np.log(l1_train_x), np.log(l1_train_y), np.log(l1_test_x), pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-6a05707fa6d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmae_stacker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml1_test_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"MAE for XGB:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmae_xgb_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"MAE for MLP:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmae_mlp_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"MAE for stacker:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmae_stacker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/metrics/regression.py\u001b[0m in \u001b[0;36mmean_absolute_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \"\"\"\n\u001b[1;32m    169\u001b[0m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[0;32m--> 170\u001b[0;31m         y_true, y_pred, multioutput)\n\u001b[0m\u001b[1;32m    171\u001b[0m     output_errors = np.average(np.abs(y_pred - y_true),\n\u001b[1;32m    172\u001b[0m                                weights=sample_weight, axis=0)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/metrics/regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    451\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[1;32m    452\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     42\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     43\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 44\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "mae_stacker = mean_absolute_error(l1_test_y, np.exp(pred))\n",
    "\n",
    "print (\"MAE for XGB:\", mae_xgb_test)\n",
    "print (\"MAE for MLP:\", mae_mlp_test)\n",
    "print (\"MAE for stacker:\", mae_stacker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
